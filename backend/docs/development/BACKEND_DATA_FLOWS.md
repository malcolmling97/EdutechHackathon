# Backend Data Flows Documentation

This document outlines all the data flows for the EdutechHackathon backend API built with Python and FastAPI, organized by functionality and resource type.

## Table of Contents

1. [Team Structure & Responsibility Split](#1-team-structure--responsibility-split)
2. [Authentication Data Flows](#2-authentication-data-flows)
3. [Folder Management Data Flows](#3-folder-management-data-flows)
4. [Space Management Data Flows](#4-space-management-data-flows)
5. [opip](#5-file-management-data-flows)
6. [Chat Data Flows](#6-chat-data-flows)
7. [Quiz Data Flows](#7-quiz-data-flows)
8. [Notes Data Flows](#8-notes-data-flows)
9. [Open-ended Questions Data Flows](#9-open-ended-questions-data-flows)
10. [Flashcards Data Flows](#10-flashcards-data-flows)
11. [Study Guides Data Flows](#11-study-guides-data-flows)
12. [Vector Database Operations](#12-vector-database-operations)
13. [Error Handling Data Flows](#13-error-handling-data-flows)
14. [Integration & Deployment Coordination](#14-integration--deployment-coordination)

---

## 1. Team Structure & Responsibility Split

### 1.1 Development Team Overview

The backend development is split between two specialized roles to enable parallel development and leverage domain expertise:

- **Backend Developer**: Core API, database, authentication, business logic
- **AI/ML Engineer**: AI integration, vector embeddings, semantic search, ML operations

### 1.2 Responsibility Matrix

| Area | Backend Developer | AI/ML Engineer | Integration Point |
|------|------------------|----------------|-------------------|
| **Core Infrastructure** | | | |
| FastAPI setup, routing | âœ… | | |
| SQLAlchemy models | âœ… | | Shared schema definitions |
| Authentication/Authorization | âœ… | | User context passing |
| Database migrations | âœ… | | Vector table schema |
| **Data Management** | | | |
| CRUD operations | âœ… | | |
| File upload/storage | âœ… | | File path/metadata passing |
| Text extraction | âœ… | | Raw text handoff |
| Vector table setup | âœ… | âœ… | Schema coordination |
| **AI/ML Operations** | | | |
| OpenAI API integration | | âœ… | |
| Embedding generation | | âœ… | File content input |
| Vector similarity search | | âœ… | Search results output |
| Prompt engineering | | âœ… | |
| **Business Logic** | | | |
| User/folder/space management | âœ… | | |
| File processing pipeline | âœ… | âœ… | Async task handoff |
| Chat session management | âœ… | âœ… | Message storage + AI response |
| Quiz/notes/flashcard CRUD | âœ… | âœ… | Content generation |
| **Integration & Testing** | | | |
| API endpoint testing | âœ… | | |
| AI service testing | | âœ… | |
| Integration testing | âœ… | âœ… | Joint responsibility |

### 1.3 Integration Contracts

**Key Interface Points:**
```python
# Backend â†’ AI Service Calls
async def generate_quiz(file_ids: List[UUID], params: QuizParams) -> QuizData
async def generate_embeddings(text: str, file_id: UUID) -> List[UUID]
async def search_similar_content(query: str, user_id: UUID) -> List[SearchResult]
async def grade_open_ended_answer(answer: str, rubric: Rubric) -> GradeResult

# AI â†’ Backend Callbacks
async def store_embeddings(embeddings: List[EmbeddingData]) -> None
async def update_file_processing_status(file_id: UUID, status: ProcessingStatus) -> None
```

### 1.4 Parallel Development Strategy

**Phase 1: Foundation (Week 1)**
- Backend: Core API endpoints, auth, database models
- AI: Service interfaces, mock implementations, vector schema design

**Phase 2: Core Features (Week 2-3)**
- Backend: File management, chat sessions, CRUD operations
- AI: Embedding pipeline, similarity search, OpenAI integration

**Phase 3: Advanced Features (Week 4-5)**
- Backend: Quiz/notes/flashcard endpoints with AI integration
- AI: Advanced prompting, grading systems, performance optimization

**Phase 4: Integration & Testing (Week 6)**
- Joint: End-to-end testing, performance tuning, deployment

### 1.5 Communication Protocols

**Daily Sync Points:**
- Morning standup: Blocker identification and resolution
- Integration checkpoints: API contract validation
- Code reviews: Cross-domain understanding

**Shared Resources:**
- API documentation (auto-generated by FastAPI)
- Database schema definitions
- Test data sets and mock responses
- Environment configuration

---

## 2. Authentication Data Flows

### 2.1 User Registration Flow

**ğŸ‘¤ Backend Developer Responsibility**

```
Frontend â†’ POST /api/v1/auth/register
â”œâ”€â”€ Pydantic schema validation (email, password, name)
â”‚   â”œâ”€â”€ Check email format with EmailStr
â”‚   â”œâ”€â”€ Validate password strength with custom validator
â”‚   â””â”€â”€ Ensure name is provided and not empty
â”œâ”€â”€ Check if user already exists in SQLAlchemy database
â”œâ”€â”€ Hash password using passlib with bcrypt (salt rounds: 12)
â”œâ”€â”€ Create user record in database
â”‚   â”œâ”€â”€ id: UUID (generated with uuid4())
â”‚   â”œâ”€â”€ email: str
â”‚   â”œâ”€â”€ password_hash: str
â”‚   â”œâ”€â”€ name: str
â”‚   â””â”€â”€ created_at: datetime
â”œâ”€â”€ Generate JWT token
â”‚   â”œâ”€â”€ payload: { sub: user_id, email: email }
â”‚   â”œâ”€â”€ secret: settings.jwt_secret_key
â”‚   â””â”€â”€ expires_delta: timedelta(hours=24)
â””â”€â”€ Return: { data: { user, token } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class UserRegistration(BaseModel):
    email: EmailStr
    password: str = Field(..., min_length=8)
    name: str = Field(..., min_length=1)
```

**Success Response:**
```json
{
  "data": {
    "user": {
      "id": "uuid",
      "email": "user@example.com",
      "name": "John Doe",
      "created_at": "2025-01-15T10:30:00Z"
    },
    "token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
  }
}
```

### 2.2 User Login Flow

**ğŸ‘¤ Backend Developer Responsibility**

```
Frontend â†’ POST /api/v1/auth/login
â”œâ”€â”€ Pydantic schema validation (email, password)
â”œâ”€â”€ Query user by email using SQLAlchemy
â”œâ”€â”€ Verify password hash using passlib.verify()
â”œâ”€â”€ Generate JWT token with python-jose
â”œâ”€â”€ Update last_login_at timestamp
â””â”€â”€ Return: { data: { user, token } }
```

**Request Body (Pydantic Schema):**
```python
class UserLogin(BaseModel):
    email: EmailStr
    password: str
```

### 2.3 User Profile Flow

```
Frontend â†’ GET /api/v1/auth/profile
â”œâ”€â”€ Extract user from JWT token (FastAPI Depends)
â”œâ”€â”€ Fetch complete user data from SQLAlchemy database
â””â”€â”€ Return: { data: { user } }
```

**Headers Required:**
```
Authorization: Bearer <jwt_token>
```

### 2.4 User Logout Flow

```
Frontend â†’ POST /api/v1/auth/logout
â”œâ”€â”€ Extract user from JWT token (FastAPI dependency)
â”œâ”€â”€ Add token to blacklist (Redis or database table)
â””â”€â”€ Return: 204 No Content
```

---

## 3. Folder Management Data Flows

**ğŸ‘¤ Backend Developer Responsibility**

### 3.1 List Folders Flow

```
Frontend â†’ GET /api/v1/folders?page=1&limit=20&q=biology
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Parse query parameters with FastAPI Query params
â”‚   â”œâ”€â”€ page: int = Query(1, ge=1)
â”‚   â”œâ”€â”€ limit: int = Query(20, ge=1, le=100)
â”‚   â””â”€â”€ q: Optional[str] = Query(None)
â”œâ”€â”€ Query database using SQLAlchemy
â”‚   â”œâ”€â”€ WHERE owner_id = user.id
â”‚   â”œâ”€â”€ AND deleted_at IS NULL
â”‚   â””â”€â”€ AND title.ilike(f'%{q}%') (if search term provided)
â”œâ”€â”€ Apply pagination with SQLAlchemy offset/limit
â”œâ”€â”€ Count total records for pagination metadata
â””â”€â”€ Return: { data: [folders], meta: { page, limit, total } }
```

**Success Response (Pydantic Schema):**
```python
class FolderListResponse(BaseModel):
    data: List[FolderResponse]
    meta: PaginationMeta

class FolderResponse(BaseModel):
    id: str
    owner_id: str
    title: str
    description: Optional[str]
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 3.2 Create Folder Flow

```
Frontend â†’ POST /api/v1/folders
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Validate request body with Pydantic
â”‚   â”œâ”€â”€ title: str (required, validated)
â”‚   â””â”€â”€ description: Optional[str]
â”œâ”€â”€ Create folder record using SQLAlchemy
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ owner_id: user.id
â”‚   â”œâ”€â”€ title: str
â”‚   â”œâ”€â”€ description: Optional[str]
â”‚   â””â”€â”€ created_at: datetime.utcnow()
â””â”€â”€ Return: { data: { folder } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class FolderCreate(BaseModel):
    title: str = Field(..., min_length=1, max_length=255)
    description: Optional[str] = Field(None, max_length=1000)
```

### 3.3 Get Folder Flow

```
Frontend â†’ GET /api/v1/folders/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Validate folder ID format (UUID validation automatic with Path param)
â”œâ”€â”€ Fetch folder from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the folder (owner_id == user.id)
â””â”€â”€ Return: { data: { folder } }
```

### 3.4 Update Folder Flow

```
Frontend â†’ PATCH /api/v1/folders/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch folder from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the folder
â”œâ”€â”€ Validate request body with Pydantic (partial update)
â”œâ”€â”€ Update folder fields in database
â””â”€â”€ Return: { data: { folder } }
```

**Request Body (Pydantic Schema):**
```python
class FolderUpdate(BaseModel):
    title: Optional[str] = Field(None, min_length=1, max_length=255)
    description: Optional[str] = Field(None, max_length=1000)
```

### 3.5 Delete Folder Flow

```
Frontend â†’ DELETE /api/v1/folders/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch folder from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the folder
â”œâ”€â”€ Perform cascade delete using SQLAlchemy relationships
â”‚   â”œâ”€â”€ Delete all spaces in folder (CASCADE)
â”‚   â”œâ”€â”€ Delete all files in folder (CASCADE)
â”‚   â”œâ”€â”€ Delete all messages in spaces (CASCADE)
â”‚   â”œâ”€â”€ Delete all quizzes in spaces (CASCADE)
â”‚   â””â”€â”€ Delete all notes in spaces (CASCADE)
â””â”€â”€ Return: 204 No Content
```

---

## 4. Space Management Data Flows

**ğŸ‘¤ Backend Developer Responsibility**

### 4.1 List Spaces in Folder Flow

```
Frontend â†’ GET /api/v1/folders/{folderId}/spaces?type=chat&page=1
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Validate folder ID format (UUID automatic validation)
â”œâ”€â”€ Verify user owns the folder
â”œâ”€â”€ Parse query parameters with FastAPI
â”‚   â”œâ”€â”€ type: Optional[SpaceType] = Query(None)
â”‚   â”œâ”€â”€ page: int = Query(1, ge=1)
â”‚   â””â”€â”€ limit: int = Query(20, ge=1, le=100)
â”œâ”€â”€ Query spaces using SQLAlchemy with relationships
â”‚   â”œâ”€â”€ WHERE folder_id = folder_id
â”‚   â”œâ”€â”€ AND deleted_at IS NULL
â”‚   â””â”€â”€ AND type = type (if filter provided)
â”œâ”€â”€ Apply pagination with SQLAlchemy
â””â”€â”€ Return: { data: [spaces], meta: { page, limit, total } }
```

**Success Response (Pydantic Schema):**
```python
class SpaceType(str, Enum):
    chat = "chat"
    quiz = "quiz"
    notes = "notes"
    openended = "openended"
    flashcards = "flashcards"
    studyguide = "studyguide"

class SpaceResponse(BaseModel):
    id: str
    folder_id: str
    type: SpaceType
    title: str
    settings: Dict[str, Any]
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 4.2 Create Space Flow

```
Frontend â†’ POST /api/v1/folders/{folderId}/spaces
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the folder
â”œâ”€â”€ Validate request body with Pydantic
â”‚   â”œâ”€â”€ type: SpaceType (enum validation)
â”‚   â”œâ”€â”€ title: str (required)
â”‚   â””â”€â”€ settings: Optional[Dict] = {}
â”œâ”€â”€ Create space record using SQLAlchemy
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ folder_id: folder_id
â”‚   â”œâ”€â”€ type: SpaceType
â”‚   â”œâ”€â”€ title: str
â”‚   â”œâ”€â”€ settings: dict (JSON field)
â”‚   â””â”€â”€ created_at: datetime.utcnow()
â””â”€â”€ Return: { data: { space } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class SpaceCreate(BaseModel):
    type: SpaceType
    title: str = Field(..., min_length=1, max_length=255)
    settings: Optional[Dict[str, Any]] = Field(default_factory=dict)
```

### 4.3 Get Space Flow

```
Frontend â†’ GET /api/v1/spaces/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch space from database using SQLAlchemy with joins
â”œâ”€â”€ Verify user owns the parent folder (through relationship)
â””â”€â”€ Return: { data: { space } }
```

### 4.4 Update Space Flow

```
Frontend â†’ PATCH /api/v1/spaces/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch space from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Validate request body with Pydantic (partial update)
â”œâ”€â”€ Update space fields using SQLAlchemy
â””â”€â”€ Return: { data: { space } }
```

### 4.5 Delete Space Flow

```
Frontend â†’ DELETE /api/v1/spaces/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch space from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Perform cascade delete using SQLAlchemy relationships
â”‚   â”œâ”€â”€ Delete all messages in space (CASCADE)
â”‚   â”œâ”€â”€ Delete all quizzes in space (CASCADE)
â”‚   â””â”€â”€ Delete all notes in space (CASCADE)
â””â”€â”€ Return: 204 No Content
```

---

## 5. File Management Data Flows

**ğŸ¤ Shared Responsibility: Backend Developer (upload/storage) + AI/ML Engineer (embedding generation)**

### 5.1 Upload Files Flow

```
Frontend â†’ POST /api/v1/files/upload (multipart/form-data)
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Validate multipart form data with FastAPI UploadFile
â”‚   â”œâ”€â”€ folder_id: str (Form field, UUID validation)
â”‚   â””â”€â”€ files: List[UploadFile] (FastAPI file handling)
â”œâ”€â”€ Verify user owns the folder
â”œâ”€â”€ Validate each file with custom validators
â”‚   â”œâ”€â”€ Check file size (max 25MB)
â”‚   â”œâ”€â”€ Validate MIME type (PDF, DOCX, TXT, MD)
â”‚   â””â”€â”€ Scan filename for security
â”œâ”€â”€ Save files to storage using async file operations
â”‚   â”œâ”€â”€ Generate unique filename with uuid4()
â”‚   â”œâ”€â”€ Save to local storage or cloud (async)
â”‚   â””â”€â”€ Get file path for database
â”œâ”€â”€ Extract text content (async background task)
â”‚   â”œâ”€â”€ PDF: Use PyPDF2 or pdfplumber
â”‚   â”œâ”€â”€ DOCX: Use python-docx
â”‚   â”œâ”€â”€ TXT/MD: Read directly with aiofiles
â”‚   â””â”€â”€ Store extracted text in PostgreSQL database
â”œâ”€â”€ ğŸ”„ **HANDOFF TO AI/ML ENGINEER** - Generate vector embeddings (async background task)
â”‚   â”œâ”€â”€ Split text into chunks (1000 chars with 200 char overlap)
â”‚   â”œâ”€â”€ Generate embeddings using OpenAI Embeddings API
â”‚   â”œâ”€â”€ Store embeddings in pgvector with metadata
â”‚   â”‚   â”œâ”€â”€ embedding: vector (1536 dimensions for text-embedding-ada-002)
â”‚   â”‚   â”œâ”€â”€ file_id: UUID (links to main file record)
â”‚   â”‚   â”œâ”€â”€ chunk_text: str (original text chunk)
â”‚   â”‚   â”œâ”€â”€ chunk_index: int (position in file)
â”‚   â”‚   â””â”€â”€ created_at: datetime
â”‚   â””â”€â”€ Link vector IDs to file record in PostgreSQL
â”œâ”€â”€ Create file records using SQLAlchemy in PostgreSQL
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ folder_id: folder_id
â”‚   â”œâ”€â”€ name: original filename
â”‚   â”œâ”€â”€ mime_type: detected MIME type
â”‚   â”œâ”€â”€ size: file size in bytes
â”‚   â”œâ”€â”€ path: storage path
â”‚   â”œâ”€â”€ text_content: extracted text
â”‚   â”œâ”€â”€ vector_ids: List[UUID] (references to pgvector embeddings)
â”‚   â””â”€â”€ created_at: datetime.utcnow()
â””â”€â”€ Return: { data: [files] } (201 Created)
```

**Request Body (FastAPI Form):**
```python
@router.post("/upload")
async def upload_files(
    folder_id: str = Form(...),
    files: List[UploadFile] = File(...),
    current_user: User = Depends(get_current_user)
):
    # Implementation
```

**Success Response (Pydantic Schema):**
```python
class FileResponse(BaseModel):
    id: str
    folder_id: str
    name: str
    mime_type: str
    size: int
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 5.2 List Files in Folder Flow

```
Frontend â†’ GET /api/v1/folders/{folderId}/files?page=1
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the folder
â”œâ”€â”€ Query files using SQLAlchemy
â”‚   â”œâ”€â”€ WHERE folder_id = folder_id
â”‚   â”œâ”€â”€ AND deleted_at IS NULL
â”‚   â””â”€â”€ ORDER BY created_at DESC
â”œâ”€â”€ Apply pagination with SQLAlchemy
â””â”€â”€ Return: { data: [files], meta: { page, limit, total } }
```

### 5.3 Get File Metadata Flow

```
Frontend â†’ GET /api/v1/files/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch file from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder (through relationship)
â””â”€â”€ Return: { data: { file } }
```

### 5.4 Get File Content Flow

```
Frontend â†’ GET /api/v1/files/{id}/content
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch file from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Check if text extraction is complete
â”œâ”€â”€ Return file content from database or storage
â””â”€â”€ Return: { data: { content, mime_type } }
```

### 5.5 Delete File Flow

```
Frontend â†’ DELETE /api/v1/files/{id}?force=false
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch file from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Check force parameter (Query param)
â”œâ”€â”€ If force=true: Hard delete
â”‚   â”œâ”€â”€ Remove file from storage using async operations
â”‚   â””â”€â”€ Delete database record
â”œâ”€â”€ If force=false: Soft delete
â”‚   â””â”€â”€ Set deleted_at timestamp
â””â”€â”€ Return: 204 No Content
```

---

## 6. Chat Data Flows

**ğŸ¤ Shared Responsibility: Backend Developer (session management) + AI/ML Engineer (AI response generation)**

### 6.1 Send Message Flow

```
Frontend â†’ POST /api/v1/spaces/{spaceId}/messages
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder (through space relationship)
â”œâ”€â”€ Validate request body with Pydantic
â”‚   â”œâ”€â”€ content: str (required, validated)
â”‚   â””â”€â”€ role: MessageRole = 'user' (enum validation)
â”œâ”€â”€ Save user message using SQLAlchemy
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ space_id: space_id
â”‚   â”œâ”€â”€ role: MessageRole.user
â”‚   â”œâ”€â”€ content: str
â”‚   â”œâ”€â”€ sources: List[Dict] = []
â”‚   â””â”€â”€ created_at: datetime.utcnow()
â”œâ”€â”€ ğŸ”„ **HANDOFF TO AI/ML ENGINEER** - Generate query embedding for similarity search
â”‚   â”œâ”€â”€ Create embedding from user message using OpenAI Embeddings API
â”‚   â””â”€â”€ Prepare embedding vector for pgvector search
â”œâ”€â”€ ğŸ¤– **AI/ML ENGINEER** - Perform vector similarity search using pgvector
â”‚   â”œâ”€â”€ Query pgvector for similar content using cosine similarity
â”‚   â”œâ”€â”€ Filter by user's accessible files (folder ownership)
â”‚   â”œâ”€â”€ Return top-k relevant chunks (k=5-10)
â”‚   â””â”€â”€ Include file metadata and chunk context
â”œâ”€â”€ ğŸ¤– **AI/ML ENGINEER** - Prepare context for AI using async operations
â”‚   â”œâ”€â”€ Get recent messages (PostgreSQL query with limit)
â”‚   â”œâ”€â”€ Get relevant file chunks from vector search results
â”‚   â”œâ”€â”€ Combine chat history with relevant context
â”‚   â””â”€â”€ Format context for AI prompt
â”œâ”€â”€ ğŸ¤– **AI/ML ENGINEER** - Call OpenAI Service using httpx async client
â”‚   â”œâ”€â”€ OpenAI API client with async httpx
â”‚   â”‚   â”œâ”€â”€ model: settings.ai_model (from config)
â”‚   â”‚   â”œâ”€â”€ messages: conversation history + context + system prompt
â”‚   â”‚   â”œâ”€â”€ max_tokens: settings.ai_max_tokens
â”‚   â”‚   â””â”€â”€ temperature: settings.ai_temperature
â”‚   â”œâ”€â”€ Process response with citations using custom logic
â”‚   â”‚   â”œâ”€â”€ Extract source references from response
â”‚   â”‚   â”œâ”€â”€ Map to file IDs, chunk indices, and pages
â”‚   â”‚   â””â”€â”€ Format citations with vector similarity scores
â”‚   â””â”€â”€ Return structured response with enhanced citations
â”œâ”€â”€ Save assistant message using SQLAlchemy in PostgreSQL
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ space_id: space_id
â”‚   â”œâ”€â”€ role: MessageRole.assistant
â”‚   â”œâ”€â”€ content: AI response
â”‚   â”œâ”€â”€ sources: citation array (JSON field with vector metadata)
â”‚   â”œâ”€â”€ vector_chunks_used: List[UUID] (pgvector chunk references)
â”‚   â”œâ”€â”€ similarity_scores: List[float] (vector search scores)
â”‚   â””â”€â”€ created_at: datetime.utcnow()
â””â”€â”€ Return: { data: { message } }
```

**Request Body (Pydantic Schema):**
```python
class MessageRole(str, Enum):
    user = "user"
    assistant = "assistant"

class MessageCreate(BaseModel):
    content: str = Field(..., min_length=1, max_length=10000)
    role: MessageRole = MessageRole.user
```

**Success Response (Pydantic Schema):**
```python
class MessageResponse(BaseModel):
    id: str
    space_id: str
    role: MessageRole
    content: str
    sources: List[Dict[str, Any]]
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 6.2 Stream Chat Reply Flow

```
Frontend â†’ POST /api/v1/spaces/{spaceId}/messages?stream=true
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Save user message to database using SQLAlchemy
â”œâ”€â”€ Set up Server-Sent Events using FastAPI StreamingResponse
â”‚   â”œâ”€â”€ Set headers: Content-Type: text/event-stream
â”‚   â”œâ”€â”€ Set headers: Cache-Control: no-cache
â”‚   â””â”€â”€ Set headers: Connection: keep-alive
â”œâ”€â”€ Call OpenAI Service with streaming using async generator
â”‚   â”œâ”€â”€ OpenAI API with stream=True using async httpx
â”‚   â”‚   â”œâ”€â”€ model: settings.ai_model
â”‚   â”‚   â”œâ”€â”€ stream: True
â”‚   â”‚   â””â”€â”€ Same parameters as non-streaming
â”‚   â”œâ”€â”€ Process streaming chunks with async for loop
â”‚   â”‚   â”œâ”€â”€ Parse OpenAI stream format
â”‚   â”‚   â”œâ”€â”€ Extract content deltas
â”‚   â”‚   â””â”€â”€ Handle citations in final chunk
â”‚   â””â”€â”€ Yield chunks using async generator
â”‚       â”œâ”€â”€ Format: "data: {json.dumps(chunk)}\n\n"
â”‚       â””â”€â”€ Yield each chunk immediately
â”œâ”€â”€ Accumulate full response in memory
â”œâ”€â”€ Save complete assistant message using SQLAlchemy
â””â”€â”€ Yield: "data: {json.dumps({'done': True})}\n\n"
```

**FastAPI Streaming Implementation:**
```python
@router.post("/messages")
async def send_message(
    space_id: str,
    message: MessageCreate,
    stream: bool = Query(False),
    current_user: User = Depends(get_current_user)
):
    if stream:
        return StreamingResponse(
            stream_ai_response(message, space_id, current_user),
            media_type="text/event-stream"
        )
    # Regular response logic

async def stream_ai_response(message: MessageCreate, space_id: str, user: User):
    async for chunk in ai_service.stream_response(message, space_id):
        yield f"data: {json.dumps(chunk)}\n\n"
```

### 6.3 Get Chat History Flow

```
Frontend â†’ GET /api/v1/spaces/{spaceId}/messages?page=1
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Query messages using SQLAlchemy
â”‚   â”œâ”€â”€ WHERE space_id = space_id
â”‚   â”œâ”€â”€ AND deleted_at IS NULL
â”‚   â””â”€â”€ ORDER BY created_at ASC
â”œâ”€â”€ Apply pagination with SQLAlchemy
â””â”€â”€ Return: { data: [messages], meta: { page, limit, total } }
```

### 6.4 Delete Message Flow

```
Frontend â†’ DELETE /api/v1/messages/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch message from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder (through relationships)
â”œâ”€â”€ Soft delete message (set deleted_at timestamp)
â””â”€â”€ Return: 204 No Content
```

---

## 7. Quiz Data Flows

**ğŸ¤ Shared Responsibility: Backend Developer (CRUD operations) + AI/ML Engineer (content generation)**

### 7.1 Generate Quiz Flow

```
Frontend â†’ POST /api/v1/spaces/{spaceId}/quizzes
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Validate request body with Pydantic
â”‚   â”œâ”€â”€ title: str (required)
â”‚   â”œâ”€â”€ file_ids: List[str] (UUID validation)
â”‚   â”œâ”€â”€ question_count: int = 10 (default, validated)
â”‚   â”œâ”€â”€ question_types: List[QuestionType] = ['mcq', 'tf']
â”‚   â””â”€â”€ difficulty: DifficultyLevel = 'medium'
â”œâ”€â”€ Fetch file contents using PostgreSQL and pgvector queries
â”‚   â”œâ”€â”€ Verify all files belong to user (ownership check)
â”‚   â”œâ”€â”€ Get text content from PostgreSQL database
â”‚   â”œâ”€â”€ Retrieve related vector embeddings from pgvector
â”‚   â”œâ”€â”€ Perform content clustering using vector similarity
â”‚   â””â”€â”€ Select diverse content chunks for quiz generation
â”œâ”€â”€ Call OpenAI Service for quiz generation using async httpx
â”‚   â”œâ”€â”€ OpenAI API with custom prompt for quiz generation
â”‚   â”‚   â”œâ”€â”€ model: settings.ai_model
â”‚   â”‚   â”œâ”€â”€ prompt: "Generate quiz questions from content..."
â”‚   â”‚   â”œâ”€â”€ parameters: question_count, types, difficulty
â”‚   â”‚   â””â”€â”€ response_format: structured JSON
â”‚   â”œâ”€â”€ Generate questions from file content using AI
â”‚   â”‚   â”œâ”€â”€ Multiple choice questions with 4 options
â”‚   â”‚   â”œâ”€â”€ True/false questions with explanations
â”‚   â”‚   â”œâ”€â”€ Short answer questions with sample answers
â”‚   â”‚   â””â”€â”€ Ensure answer accuracy and relevance
â”‚   â”œâ”€â”€ Parse and validate AI response structure
â”‚   â””â”€â”€ Return structured quiz object
â”œâ”€â”€ Save quiz using SQLAlchemy
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ space_id: space_id
â”‚   â”œâ”€â”€ title: str
â”‚   â”œâ”€â”€ questions: List[Dict] (JSON field with questions)
â”‚   â”œâ”€â”€ file_ids: List[str] (JSON field with source files)
â”‚   â””â”€â”€ created_at: datetime.utcnow()
â””â”€â”€ Return: { data: { quiz } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class QuestionType(str, Enum):
    mcq = "mcq"
    tf = "tf"
    short_answer = "short_answer"

class DifficultyLevel(str, Enum):
    easy = "easy"
    medium = "medium"
    hard = "hard"

class QuizCreate(BaseModel):
    title: str = Field(..., min_length=1, max_length=255)
    file_ids: List[str] = Field(..., min_items=1)
    question_count: int = Field(10, ge=1, le=50)
    question_types: List[QuestionType] = Field(default=[QuestionType.mcq, QuestionType.tf])
    difficulty: DifficultyLevel = DifficultyLevel.medium
```

**Success Response (Pydantic Schema):**
```python
class QuizResponse(BaseModel):
    id: str
    space_id: str
    title: str
    questions: List[Dict[str, Any]]
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 7.2 List Quizzes Flow

```
Frontend â†’ GET /api/v1/spaces/{spaceId}/quizzes?page=1
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Query quizzes using SQLAlchemy
â”‚   â”œâ”€â”€ WHERE space_id = space_id
â”‚   â”œâ”€â”€ AND deleted_at IS NULL
â”‚   â””â”€â”€ ORDER BY created_at DESC
â”œâ”€â”€ Apply pagination with SQLAlchemy
â””â”€â”€ Return: { data: [quizzes], meta: { page, limit, total } }
```

### 7.3 Get Quiz Detail Flow

```
Frontend â†’ GET /api/v1/quizzes/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch quiz from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder (through relationships)
â””â”€â”€ Return: { data: { quiz } }
```

### 7.4 Submit Quiz Answers Flow

```
Frontend â†’ POST /api/v1/quizzes/{id}/submit
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch quiz from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Validate submitted answers with Pydantic
â”‚   â”œâ”€â”€ Check answer format for each question type
â”‚   â”œâ”€â”€ Ensure all required questions answered
â”‚   â””â”€â”€ Validate answer choices exist for MCQ
â”œâ”€â”€ Grade answers using AI service and custom logic
â”‚   â”œâ”€â”€ Grade MCQ and T&F automatically with Python logic
â”‚   â”œâ”€â”€ Grade short answers using OpenAI API
â”‚   â”‚   â”œâ”€â”€ Call OpenAI API for each short answer
â”‚   â”‚   â”œâ”€â”€ Compare with expected answer using AI
â”‚   â”‚   â””â”€â”€ Assign partial credit based on AI evaluation
â”‚   â”œâ”€â”€ Calculate total score using Python calculations
â”‚   â”œâ”€â”€ Generate feedback for each question
â”‚   â””â”€â”€ Return grading results
â”œâ”€â”€ Save submission using SQLAlchemy
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ quiz_id: quiz_id
â”‚   â”œâ”€â”€ user_id: user.id
â”‚   â”œâ”€â”€ answers: List[Dict] (JSON field)
â”‚   â”œâ”€â”€ score: float
â”‚   â”œâ”€â”€ feedback: List[Dict] (JSON field)
â”‚   â””â”€â”€ submitted_at: datetime.utcnow()
â””â”€â”€ Return: { data: { score, feedback, answers } }
```

**Request Body (Pydantic Schema):**
```python
class QuizAnswer(BaseModel):
    question_id: str
    answer: Union[str, int]  # String for text, int for choice index

class QuizSubmission(BaseModel):
    answers: List[QuizAnswer] = Field(..., min_items=1)
```

**Success Response (Pydantic Schema):**
```python
class QuizSubmissionResponse(BaseModel):
    score: float
    total_questions: int
    feedback: List[Dict[str, Any]]
    submitted_at: datetime
```

### 7.5 Delete Quiz Flow

```
Frontend â†’ DELETE /api/v1/quizzes/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch quiz from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Delete quiz and all submissions using SQLAlchemy CASCADE
â””â”€â”€ Return: 204 No Content
```

---

## 8. Notes Data Flows

### 8.1 Generate Notes Flow

```
Frontend â†’ POST /api/v1/spaces/{spaceId}/notes
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Validate request body with Pydantic
â”‚   â”œâ”€â”€ file_ids: List[str] (UUID validation, required)
â”‚   â””â”€â”€ format: NotesFormat = 'markdown' (enum validation)
â”œâ”€â”€ Fetch file contents using PostgreSQL and pgvector queries
â”‚   â”œâ”€â”€ Verify all files belong to user (ownership validation)
â”‚   â”œâ”€â”€ Get text content from PostgreSQL database
â”‚   â”œâ”€â”€ Retrieve related vector embeddings from pgvector
â”‚   â”œâ”€â”€ Perform semantic clustering of content using vector similarity
â”‚   â””â”€â”€ Organize content chunks by topic for structured notes
â”œâ”€â”€ Call OpenAI Service for notes generation using async httpx
â”‚   â”œâ”€â”€ OpenAI API with custom prompt for note generation
â”‚   â”‚   â”œâ”€â”€ model: settings.ai_model
â”‚   â”‚   â”œâ”€â”€ prompt: "Generate summary notes from content..."
â”‚   â”‚   â”œâ”€â”€ parameters: format preference
â”‚   â”‚   â””â”€â”€ response_format: structured text
â”‚   â”œâ”€â”€ Generate notes from file content using AI
â”‚   â”‚   â”œâ”€â”€ Extract key concepts and important information
â”‚   â”‚   â”œâ”€â”€ Organize information logically by topics
â”‚   â”‚   â”œâ”€â”€ Include important details and examples
â”‚   â”‚   â””â”€â”€ Format according to user preference
â”‚   â”œâ”€â”€ Format output as markdown or bullet points
â”‚   â””â”€â”€ Return structured notes object
â”œâ”€â”€ Save notes using SQLAlchemy
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ space_id: space_id
â”‚   â”œâ”€â”€ format: NotesFormat
â”‚   â”œâ”€â”€ content: str (formatted notes content)
â”‚   â”œâ”€â”€ file_ids: List[str] (JSON field with source files)
â”‚   â””â”€â”€ created_at: datetime.utcnow()
â””â”€â”€ Return: { data: { notes } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class NotesFormat(str, Enum):
    markdown = "markdown"
    bullet = "bullet"

class NotesCreate(BaseModel):
    file_ids: List[str] = Field(..., min_items=1)
    format: NotesFormat = NotesFormat.markdown
```

**Success Response (Pydantic Schema):**
```python
class NotesResponse(BaseModel):
    id: str
    space_id: str
    format: NotesFormat
    content: str
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 8.2 List Notes Flow

```
Frontend â†’ GET /api/v1/spaces/{spaceId}/notes?page=1
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Query notes using SQLAlchemy
â”‚   â”œâ”€â”€ WHERE space_id = space_id
â”‚   â”œâ”€â”€ AND deleted_at IS NULL
â”‚   â””â”€â”€ ORDER BY created_at DESC
â”œâ”€â”€ Apply pagination with SQLAlchemy
â””â”€â”€ Return: { data: [notes], meta: { page, limit, total } }
```

### 8.3 Get Note Flow

```
Frontend â†’ GET /api/v1/notes/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch note from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder (through relationships)
â””â”€â”€ Return: { data: { note } }
```

### 8.4 Update Note Flow

```
Frontend â†’ PATCH /api/v1/notes/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch note from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Validate request body with Pydantic (partial update)
â”œâ”€â”€ Update note content using SQLAlchemy
â”œâ”€â”€ Update updated_at timestamp
â””â”€â”€ Return: { data: { note } }
```

**Request Body (Pydantic Schema):**
```python
class NotesUpdate(BaseModel):
    content: Optional[str] = Field(None, min_length=1)
```

### 8.5 Delete Note Flow

```
Frontend â†’ DELETE /api/v1/notes/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch note from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â””â”€â”€ Return: 204 No Content
```

---

## 9. Open-ended Questions Data Flows

**ğŸ¤ Shared Responsibility: Backend Developer (CRUD operations) + AI/ML Engineer (question generation & grading)**

### 9.1 Generate Open-ended Question Flow

```
Frontend â†’ POST /api/v1/spaces/{spaceId}/open-ended-questions
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Validate request body with Pydantic
â”‚   â”œâ”€â”€ title: str (required)
â”‚   â”œâ”€â”€ file_ids: List[str] (UUID validation, required)
â”‚   â”œâ”€â”€ question_count: int = 10 (default, validated)
â”‚   â”œâ”€â”€ question_types: List[QuestionType] = ['short_answer']
â”‚   â””â”€â”€ difficulty: DifficultyLevel = 'medium'
â”œâ”€â”€ Fetch file contents using PostgreSQL and pgvector queries
â”‚   â”œâ”€â”€ Verify all files belong to user (ownership check)
â”‚   â”œâ”€â”€ Get text content from PostgreSQL database
â”‚   â”œâ”€â”€ Retrieve related vector embeddings from pgvector
â”‚   â”œâ”€â”€ Perform semantic analysis using vector similarity
â”‚   â””â”€â”€ Select diverse content chunks for question generation
â”œâ”€â”€ Call OpenAI Service for open-ended question generation using async httpx
â”‚   â”œâ”€â”€ OpenAI API with custom prompt for open-ended question generation
â”‚   â”‚   â”œâ”€â”€ model: settings.ai_model
â”‚   â”‚   â”œâ”€â”€ prompt: "Generate open-ended questions from content..."
â”‚   â”‚   â”œâ”€â”€ parameters: question_count, types, difficulty
â”‚   â”‚   â””â”€â”€ response_format: structured JSON
â”‚   â”œâ”€â”€ Generate questions from file content using AI
â”‚   â”‚   â”œâ”€â”€ Short answer questions with sample answers
â”‚   â”‚   â”œâ”€â”€ Ensure answer accuracy and relevance
â”‚   â”‚   â””â”€â”€ Format according to user preference
â”‚   â”œâ”€â”€ Parse and validate AI response structure
â”‚   â””â”€â”€ Return structured question object
â”œâ”€â”€ Save open-ended question using SQLAlchemy
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ space_id: space_id
â”‚   â”œâ”€â”€ title: str
â”‚   â”œâ”€â”€ questions: List[Dict] (JSON field with questions)
â”‚   â”œâ”€â”€ file_ids: List[str] (JSON field with source files)
â”‚   â””â”€â”€ created_at: datetime.utcnow()
â””â”€â”€ Return: { data: { open_ended_question } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class OpenEndedQuestionType(str, Enum):
    short_answer = "short_answer"

class OpenEndedDifficultyLevel(str, Enum):
    easy = "easy"
    medium = "medium"
    hard = "hard"

class OpenEndedQuestionCreate(BaseModel):
    title: str = Field(..., min_length=1, max_length=255)
    file_ids: List[str] = Field(..., min_items=1)
    question_count: int = Field(10, ge=1, le=50)
    question_types: List[OpenEndedQuestionType] = Field(default=[OpenEndedQuestionType.short_answer])
    difficulty: OpenEndedDifficultyLevel = OpenEndedDifficultyLevel.medium
```

**Success Response (Pydantic Schema):**
```python
class OpenEndedQuestionResponse(BaseModel):
    id: str
    space_id: str
    title: str
    questions: List[Dict[str, Any]]
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 8.2 List Open-ended Questions Flow

```
Frontend â†’ GET /api/v1/spaces/{spaceId}/open-ended-questions?page=1
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Query open-ended questions using SQLAlchemy
â”‚   â”œâ”€â”€ WHERE space_id = space_id
â”‚   â”œâ”€â”€ AND deleted_at IS NULL
â”‚   â””â”€â”€ ORDER BY created_at DESC
â”œâ”€â”€ Apply pagination with SQLAlchemy
â””â”€â”€ Return: { data: [open_ended_questions], meta: { page, limit, total } }
```

### 8.3 Get Open-ended Question Detail Flow

```
Frontend â†’ GET /api/v1/open-ended-questions/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch open-ended question from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder (through relationships)
â””â”€â”€ Return: { data: { open_ended_question } }
```

### 8.4 Submit Open-ended Question Answers Flow

```
Frontend â†’ POST /api/v1/open-ended-questions/{id}/submit
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch open-ended question from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Validate submitted answers with Pydantic
â”‚   â”œâ”€â”€ Check answer format for each question type
â”‚   â”œâ”€â”€ Ensure all required questions answered
â”‚   â””â”€â”€ Validate answer choices exist for MCQ
â”œâ”€â”€ Grade answers using AI service and custom logic
â”‚   â”œâ”€â”€ Grade MCQ and T&F automatically with Python logic
â”‚   â”œâ”€â”€ Grade short answers using OpenAI API
â”‚   â”‚   â”œâ”€â”€ Call OpenAI API for each short answer
â”‚   â”‚   â”œâ”€â”€ Compare with expected answer using AI
â”‚   â”‚   â””â”€â”€ Assign partial credit based on AI evaluation
â”‚   â”œâ”€â”€ Calculate total score using Python calculations
â”‚   â”œâ”€â”€ Generate feedback for each question
â”‚   â””â”€â”€ Return grading results
â”œâ”€â”€ Save submission using SQLAlchemy
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ open_ended_question_id: open_ended_question_id
â”‚   â”œâ”€â”€ user_id: user.id
â”‚   â”œâ”€â”€ answers: List[Dict] (JSON field)
â”‚   â”œâ”€â”€ score: float
â”‚   â”œâ”€â”€ feedback: List[Dict] (JSON field)
â”‚   â””â”€â”€ submitted_at: datetime.utcnow()
â””â”€â”€ Return: { data: { score, feedback, answers } }
```

**Request Body (Pydantic Schema):**
```python
class OpenEndedQuestionAnswer(BaseModel):
    question_id: str
    answer: str  # String for short answer

class OpenEndedQuestionSubmission(BaseModel):
    answers: List[OpenEndedQuestionAnswer] = Field(..., min_items=1)
```

**Success Response (Pydantic Schema):**
```python
class OpenEndedQuestionSubmissionResponse(BaseModel):
    score: float
    total_questions: int
    feedback: List[Dict[str, Any]]
    submitted_at: datetime
```

### 8.5 Delete Open-ended Question Flow

```
Frontend â†’ DELETE /api/v1/open-ended-questions/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch open-ended question from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Delete open-ended question and all submissions using SQLAlchemy CASCADE
â””â”€â”€ Return: 204 No Content
```

---

## 10. Flashcards Data Flows

### 10.1 Generate Flashcards Flow

```
Frontend â†’ POST /api/v1/spaces/{spaceId}/flashcards
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Validate request body with Pydantic
â”‚   â”œâ”€â”€ title: str (required)
â”‚   â”œâ”€â”€ file_ids: List[str] (UUID validation, required)
â”‚   â”œâ”€â”€ card_count: int = 20 (default, validated)
â”‚   â”œâ”€â”€ card_types: List[CardType] = ['mcq', 'tf']
â”‚   â””â”€â”€ difficulty: DifficultyLevel = 'medium'
â”œâ”€â”€ Fetch file contents using PostgreSQL and pgvector queries
â”‚   â”œâ”€â”€ Verify all files belong to user (ownership check)
â”‚   â”œâ”€â”€ Get text content from PostgreSQL database
â”‚   â”œâ”€â”€ Retrieve related vector embeddings from pgvector
â”‚   â”œâ”€â”€ Identify key terms and concepts using vector clustering
â”‚   â””â”€â”€ Select optimal content chunks for flashcard generation
â”œâ”€â”€ Call OpenAI Service for flashcard generation using async httpx
â”‚   â”œâ”€â”€ OpenAI API with custom prompt for flashcard generation
â”‚   â”‚   â”œâ”€â”€ model: settings.ai_model
â”‚   â”‚   â”œâ”€â”€ prompt: "Generate flashcards from content..."
â”‚   â”‚   â”œâ”€â”€ parameters: card_count, types, difficulty
â”‚   â”‚   â””â”€â”€ response_format: structured JSON
â”‚   â”œâ”€â”€ Generate flashcards from file content using AI
â”‚   â”‚   â”œâ”€â”€ Multiple choice questions with 4 options
â”‚   â”‚   â”œâ”€â”€ True/false questions with explanations
â”‚   â”‚   â”œâ”€â”€ Short answer questions with sample answers
â”‚   â”‚   â””â”€â”€ Ensure answer accuracy and relevance
â”‚   â”œâ”€â”€ Parse and validate AI response structure
â”‚   â””â”€â”€ Return structured flashcard object
â”œâ”€â”€ Save flashcard using SQLAlchemy
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ space_id: space_id
â”‚   â”œâ”€â”€ title: str
â”‚   â”œâ”€â”€ flashcards: List[Dict] (JSON field with flashcards)
â”‚   â”œâ”€â”€ file_ids: List[str] (JSON field with source files)
â”‚   â””â”€â”€ created_at: datetime.utcnow()
â””â”€â”€ Return: { data: { flashcard } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class CardType(str, Enum):
    mcq = "mcq"
    tf = "tf"
    short_answer = "short_answer"

class FlashcardDifficultyLevel(str, Enum):
    easy = "easy"
    medium = "medium"
    hard = "hard"

class FlashcardCreate(BaseModel):
    title: str = Field(..., min_length=1, max_length=255)
    file_ids: List[str] = Field(..., min_items=1)
    card_count: int = Field(20, ge=1, le=100)
    card_types: List[CardType] = Field(default=[CardType.mcq, CardType.tf])
    difficulty: FlashcardDifficultyLevel = FlashcardDifficultyLevel.medium
```

**Success Response (Pydantic Schema):**
```python
class FlashcardResponse(BaseModel):
    id: str
    space_id: str
    title: str
    flashcards: List[Dict[str, Any]]
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 10.2 List Flashcards Flow

```
Frontend â†’ GET /api/v1/spaces/{spaceId}/flashcards?page=1
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Query flashcards using SQLAlchemy
â”‚   â”œâ”€â”€ WHERE space_id = space_id
â”‚   â”œâ”€â”€ AND deleted_at IS NULL
â”‚   â””â”€â”€ ORDER BY created_at DESC
â”œâ”€â”€ Apply pagination with SQLAlchemy
â””â”€â”€ Return: { data: [flashcards], meta: { page, limit, total } }
```

### 10.3 Get Flashcard Detail Flow

```
Frontend â†’ GET /api/v1/flashcards/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch flashcard from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder (through relationships)
â””â”€â”€ Return: { data: { flashcard } }
```

### 10.4 Submit Flashcard Answers Flow

```
Frontend â†’ POST /api/v1/flashcards/{id}/submit
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch flashcard from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Validate submitted answers with Pydantic
â”‚   â”œâ”€â”€ Check answer format for each question type
â”‚   â”œâ”€â”€ Ensure all required questions answered
â”‚   â””â”€â”€ Validate answer choices exist for MCQ
â”œâ”€â”€ Grade answers using AI service and custom logic
â”‚   â”œâ”€â”€ Grade MCQ and T&F automatically with Python logic
â”‚   â”œâ”€â”€ Grade short answers using OpenAI API
â”‚   â”‚   â”œâ”€â”€ Call OpenAI API for each short answer
â”‚   â”‚   â”œâ”€â”€ Compare with expected answer using AI
â”‚   â”‚   â””â”€â”€ Assign partial credit based on AI evaluation
â”‚   â”œâ”€â”€ Calculate total score using Python calculations
â”‚   â”œâ”€â”€ Generate feedback for each question
â”‚   â””â”€â”€ Return grading results
â”œâ”€â”€ Save submission using SQLAlchemy
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ flashcard_id: flashcard_id
â”‚   â”œâ”€â”€ user_id: user.id
â”‚   â”œâ”€â”€ answers: List[Dict] (JSON field)
â”‚   â”œâ”€â”€ score: float
â”‚   â”œâ”€â”€ feedback: List[Dict] (JSON field)
â”‚   â””â”€â”€ submitted_at: datetime.utcnow()
â””â”€â”€ Return: { data: { score, feedback, answers } }
```

**Request Body (Pydantic Schema):**
```python
class FlashcardAnswer(BaseModel):
    question_id: str
    answer: Union[str, int]  # String for text, int for choice index

class FlashcardSubmission(BaseModel):
    answers: List[FlashcardAnswer] = Field(..., min_items=1)
```

**Success Response (Pydantic Schema):**
```python
class FlashcardSubmissionResponse(BaseModel):
    score: float
    total_questions: int
    feedback: List[Dict[str, Any]]
    submitted_at: datetime
```

### 10.5 Delete Flashcard Flow

```
Frontend â†’ DELETE /api/v1/flashcards/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch flashcard from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Delete flashcard and all submissions using SQLAlchemy CASCADE
â””â”€â”€ Return: 204 No Content
```

---

## 11. Study Guides Data Flows

### 11.1 Generate Study Guide Flow

```
Frontend â†’ POST /api/v1/spaces/{spaceId}/study-guides
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Validate request body with Pydantic
â”‚   â”œâ”€â”€ title: str (required)
â”‚   â”œâ”€â”€ file_ids: List[str] (UUID validation, required)
â”‚   â”œâ”€â”€ guide_type: GuideType = 'summary' (enum validation)
â”‚   â””â”€â”€ difficulty: DifficultyLevel = 'medium'
â”œâ”€â”€ Fetch file contents using PostgreSQL and pgvector queries
â”‚   â”œâ”€â”€ Verify all files belong to user (ownership check)
â”‚   â”œâ”€â”€ Get text content from PostgreSQL database
â”‚   â”œâ”€â”€ Retrieve related vector embeddings from pgvector
â”‚   â”œâ”€â”€ Analyze content structure using vector similarity
â”‚   â”œâ”€â”€ Create topic hierarchy from vector clusters
â”‚   â””â”€â”€ Organize content for structured study guide generation
â”œâ”€â”€ Call OpenAI Service for study guide generation using async httpx
â”‚   â”œâ”€â”€ OpenAI API with custom prompt for study guide generation
â”‚   â”‚   â”œâ”€â”€ model: settings.ai_model
â”‚   â”‚   â”œâ”€â”€ prompt: "Generate study guide from content..."
â”‚   â”‚   â”œâ”€â”€ parameters: guide_type, difficulty
â”‚   â”‚   â””â”€â”€ response_format: structured text
â”‚   â”œâ”€â”€ Generate study guide from file content using AI
â”‚   â”‚   â”œâ”€â”€ Extract key concepts and important information
â”‚   â”‚   â”œâ”€â”€ Organize information logically by topics
â”‚   â”‚   â”œâ”€â”€ Include important details and examples
â”‚   â”‚   â””â”€â”€ Format according to user preference
â”‚   â”œâ”€â”€ Format output as markdown or bullet points
â”‚   â””â”€â”€ Return structured study guide object
â”œâ”€â”€ Save study guide using SQLAlchemy
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ space_id: space_id
â”‚   â”œâ”€â”€ title: str
â”‚   â”œâ”€â”€ guide_type: GuideType
â”‚   â”œâ”€â”€ content: str (formatted guide content)
â”‚   â”œâ”€â”€ file_ids: List[str] (JSON field with source files)
â”‚   â””â”€â”€ created_at: datetime.utcnow()
â””â”€â”€ Return: { data: { study_guide } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class GuideType(str, Enum):
    summary = "summary"
    detailed = "detailed"

class StudyGuideDifficultyLevel(str, Enum):
    easy = "easy"
    medium = "medium"
    hard = "hard"

class StudyGuideCreate(BaseModel):
    title: str = Field(..., min_length=1, max_length=255)
    file_ids: List[str] = Field(..., min_items=1)
    guide_type: GuideType = GuideType.summary
    difficulty: StudyGuideDifficultyLevel = StudyGuideDifficultyLevel.medium
```

**Success Response (Pydantic Schema):**
```python
class StudyGuideResponse(BaseModel):
    id: str
    space_id: str
    title: str
    guide_type: GuideType
    content: str
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 11.2 List Study Guides Flow

```
Frontend â†’ GET /api/v1/spaces/{spaceId}/study-guides?page=1
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Query study guides using SQLAlchemy
â”‚   â”œâ”€â”€ WHERE space_id = space_id
â”‚   â”œâ”€â”€ AND deleted_at IS NULL
â”‚   â””â”€â”€ ORDER BY created_at DESC
â”œâ”€â”€ Apply pagination with SQLAlchemy
â””â”€â”€ Return: { data: [study_guides], meta: { page, limit, total } }
```

### 11.3 Get Study Guide Detail Flow

```
Frontend â†’ GET /api/v1/study-guides/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch study guide from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder (through relationships)
â””â”€â”€ Return: { data: { study_guide } }
```

### 11.4 Submit Study Guide Answers Flow

```
Frontend â†’ POST /api/v1/study-guides/{id}/submit
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch study guide from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Validate submitted answers with Pydantic
â”‚   â”œâ”€â”€ Check answer format for each question type
â”‚   â”œâ”€â”€ Ensure all required questions answered
â”‚   â””â”€â”€ Validate answer choices exist for MCQ
â”œâ”€â”€ Grade answers using AI service and custom logic
â”‚   â”œâ”€â”€ Grade MCQ and T&F automatically with Python logic
â”‚   â”œâ”€â”€ Grade short answers using OpenAI API
â”‚   â”‚   â”œâ”€â”€ Call OpenAI API for each short answer
â”‚   â”‚   â”œâ”€â”€ Compare with expected answer using AI
â”‚   â”‚   â””â”€â”€ Assign partial credit based on AI evaluation
â”‚   â”œâ”€â”€ Calculate total score using Python calculations
â”‚   â”œâ”€â”€ Generate feedback for each question
â”‚   â””â”€â”€ Return grading results
â”œâ”€â”€ Save submission using SQLAlchemy
â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”œâ”€â”€ study_guide_id: study_guide_id
â”‚   â”œâ”€â”€ user_id: user.id
â”‚   â”œâ”€â”€ answers: List[Dict] (JSON field)
â”‚   â”œâ”€â”€ score: float
â”‚   â”œâ”€â”€ feedback: List[Dict] (JSON field)
â”‚   â””â”€â”€ submitted_at: datetime.utcnow()
â””â”€â”€ Return: { data: { score, feedback, answers } }
```

**Request Body (Pydantic Schema):**
```python
class StudyGuideAnswer(BaseModel):
    question_id: str
    answer: str  # String for short answer

class StudyGuideSubmission(BaseModel):
    answers: List[StudyGuideAnswer] = Field(..., min_items=1)
```

**Success Response (Pydantic Schema):**
```python
class StudyGuideSubmissionResponse(BaseModel):
    score: float
    total_questions: int
    feedback: List[Dict[str, Any]]
    submitted_at: datetime
```

### 11.5 Delete Study Guide Flow

```
Frontend â†’ DELETE /api/v1/study-guides/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch study guide from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the parent folder
â”œâ”€â”€ Delete study guide and all submissions using SQLAlchemy CASCADE
â””â”€â”€ Return: 204 No Content
```

---

## 12. Vector Database Operations

**ğŸ¤– Primary AI/ML Engineer Responsibility (with Backend Developer coordination on schema)**

### 12.1 Database Architecture

**PostgreSQL + pgvector Setup:**
- **PostgreSQL**: Stores structured data (users, folders, files, chat sessions, metadata)
- **pgvector**: Extension for storing and querying vector embeddings
- **Linkage**: Chat session IDs and file IDs link PostgreSQL records to vector embeddings

### 12.2 Text Chunking and Embedding Flow

```
Internal Process â†’ Text Processing Pipeline
â”œâ”€â”€ Receive text content from file upload or message
â”œâ”€â”€ Text chunking with overlap strategy
â”‚   â”œâ”€â”€ Split text into chunks (1000 characters each)
â”‚   â”œâ”€â”€ Add overlap between chunks (200 characters)
â”‚   â”œâ”€â”€ Preserve sentence boundaries where possible
â”‚   â””â”€â”€ Handle edge cases (tables, code blocks, etc.)
â”œâ”€â”€ Generate embeddings using OpenAI API
â”‚   â”œâ”€â”€ Call OpenAI Embeddings API (text-embedding-ada-002)
â”‚   â”œâ”€â”€ Model: "text-embedding-ada-002" (1536 dimensions)
â”‚   â”œâ”€â”€ Batch process multiple chunks for efficiency
â”‚   â””â”€â”€ Handle rate limiting and retry logic
â”œâ”€â”€ Store embeddings in pgvector
â”‚   â”œâ”€â”€ Create vector record with pgvector
â”‚   â”‚   â”œâ”€â”€ id: UUID (generated)
â”‚   â”‚   â”œâ”€â”€ embedding: vector(1536) (pgvector type)
â”‚   â”‚   â”œâ”€â”€ content: text (original chunk text)
â”‚   â”‚   â”œâ”€â”€ file_id: UUID (links to PostgreSQL file record)
â”‚   â”‚   â”œâ”€â”€ chunk_index: int (position in original text)
â”‚   â”‚   â”œâ”€â”€ metadata: JSONB (additional context)
â”‚   â”‚   â””â”€â”€ created_at: timestamp
â”‚   â””â”€â”€ Create index for fast similarity search
â”œâ”€â”€ Update PostgreSQL file record
â”‚   â”œâ”€â”€ Add vector_ids: List[UUID] (references to pgvector)
â”‚   â”œâ”€â”€ Update processing_status: 'completed'
â”‚   â””â”€â”€ Update vectors_count: int (number of chunks)
â””â”€â”€ Return processing results for logging
```

### 12.3 Vector Similarity Search Flow

```
Internal Process â†’ Semantic Search
â”œâ”€â”€ Receive search query from chat/quiz/notes generation
â”œâ”€â”€ Generate query embedding
â”‚   â”œâ”€â”€ Call OpenAI Embeddings API with user query
â”‚   â”œâ”€â”€ Use same model as document embeddings
â”‚   â””â”€â”€ Return query vector (1536 dimensions)
â”œâ”€â”€ Perform pgvector similarity search
â”‚   â”œâ”€â”€ Use cosine similarity operator (<=>)
â”‚   â”œâ”€â”€ Filter by user access permissions
â”‚   â”‚   â”œâ”€â”€ JOIN with PostgreSQL file ownership
â”‚   â”‚   â”œâ”€â”€ WHERE files.folder.owner_id = user.id
â”‚   â”‚   â””â”€â”€ AND files.deleted_at IS NULL
â”‚   â”œâ”€â”€ Apply similarity threshold (e.g., > 0.7)
â”‚   â”œâ”€â”€ Limit results (top-k, typically k=5-10)
â”‚   â””â”€â”€ ORDER BY similarity score DESC
â”œâ”€â”€ Fetch related metadata from PostgreSQL
â”‚   â”œâ”€â”€ JOIN vector results with file metadata
â”‚   â”œâ”€â”€ Include file names, types, creation dates
â”‚   â””â”€â”€ Aggregate chunks by file for context
â”œâ”€â”€ Prepare context for AI service
â”‚   â”œâ”€â”€ Combine relevant chunks with metadata
â”‚   â”œâ”€â”€ Include similarity scores for citation weighting
â”‚   â””â”€â”€ Format for LLM consumption
â””â”€â”€ Return search results with metadata
```

**pgvector SQL Example:**
```sql
SELECT 
    v.id,
    v.content,
    v.file_id,
    v.chunk_index,
    f.name as file_name,
    1 - (v.embedding <=> $1) as similarity_score
FROM vector_embeddings v
JOIN files f ON v.file_id = f.id
JOIN folders fo ON f.folder_id = fo.id
WHERE fo.owner_id = $2
AND (1 - (v.embedding <=> $1)) > 0.7
ORDER BY v.embedding <=> $1
LIMIT 10;
```

### 12.4 Chat Session Vector Linking Flow

```
Chat Message Flow â†’ Vector Context Integration
â”œâ”€â”€ User sends message to chat space
â”œâ”€â”€ Generate message embedding for similarity search
â”œâ”€â”€ Perform vector similarity search (as above)
â”œâ”€â”€ Store chat session data in PostgreSQL
â”‚   â”œâ”€â”€ message_id: UUID (primary key)
â”‚   â”œâ”€â”€ space_id: UUID (links to space)
â”‚   â”œâ”€â”€ user_id: UUID (message sender)
â”‚   â”œâ”€â”€ content: text (message content)
â”‚   â”œâ”€â”€ role: enum ('user' | 'assistant')
â”‚   â”œâ”€â”€ vector_chunks_used: UUID[] (pgvector references)
â”‚   â”œâ”€â”€ similarity_scores: float[] (matching scores)
â”‚   â””â”€â”€ created_at: timestamp
â”œâ”€â”€ Link vector chunks to chat session
â”‚   â”œâ”€â”€ Create chat_vector_usage table entries
â”‚   â”‚   â”œâ”€â”€ chat_message_id: UUID (FK to messages)
â”‚   â”‚   â”œâ”€â”€ vector_embedding_id: UUID (FK to pgvector)
â”‚   â”‚   â”œâ”€â”€ similarity_score: float
â”‚   â”‚   â”œâ”€â”€ usage_type: enum ('context' | 'citation')
â”‚   â”‚   â””â”€â”€ created_at: timestamp
â”‚   â””â”€â”€ Enable traceability from chat to source content
â””â”€â”€ Return AI response with linked vector references
```

### 12.5 Vector Database Maintenance Flow

```
Background Tasks â†’ Vector Database Optimization
â”œâ”€â”€ Periodic vector cleanup
â”‚   â”œâ”€â”€ Remove orphaned vectors (files deleted)
â”‚   â”œâ”€â”€ Cleanup old chat session vector links
â”‚   â””â”€â”€ Vacuum and analyze pgvector indexes
â”œâ”€â”€ Index optimization
â”‚   â”œâ”€â”€ Rebuild pgvector indexes periodically
â”‚   â”œâ”€â”€ Update index parameters for performance
â”‚   â””â”€â”€ Monitor index usage statistics
â”œâ”€â”€ Embedding model updates
â”‚   â”œâ”€â”€ Handle embedding model changes
â”‚   â”œâ”€â”€ Migrate to new embedding dimensions
â”‚   â””â”€â”€ Batch re-embed existing content
â””â”€â”€ Performance monitoring
    â”œâ”€â”€ Track vector search latency
    â”œâ”€â”€ Monitor embedding generation costs
    â””â”€â”€ Optimize chunk size and overlap parameters
```

**Database Schema (SQLAlchemy Models):**
```python
# PostgreSQL Models
class VectorEmbedding(Base):
    __tablename__ = "vector_embeddings"
    
    id: UUID = Column(UUID, primary_key=True)
    embedding = Column(Vector(1536))  # pgvector type
    content: str = Column(Text)
    file_id: UUID = Column(UUID, ForeignKey("files.id"))
    chunk_index: int = Column(Integer)
    metadata: dict = Column(JSONB)
    created_at: datetime = Column(DateTime)

class ChatVectorUsage(Base):
    __tablename__ = "chat_vector_usage"
    
    id: UUID = Column(UUID, primary_key=True)
    chat_message_id: UUID = Column(UUID, ForeignKey("chat_messages.id"))
    vector_embedding_id: UUID = Column(UUID, ForeignKey("vector_embeddings.id"))
    similarity_score: float = Column(Float)
    usage_type: str = Column(Enum(UsageType))
    created_at: datetime = Column(DateTime)
```

**Request Body (Pydantic Schema):**
```python
class SearchEmbeddings(BaseModel):
    query_text: str = Field(..., min_length=1)
    model_name: Optional[str] = Field(None, max_length=255)
```

**Success Response (Pydantic Schema):**
```python
class SearchEmbeddingsResponse(BaseModel):
    search_results: List[Dict[str, Any]]
    user_id: str
    query_text: str
    model_name: str
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 12.6 Delete Embedding Flow

```
Frontend â†’ DELETE /api/v1/vector-db/embeddings/{id}
â”œâ”€â”€ Extract user from JWT token (Depends(get_current_user))
â”œâ”€â”€ Fetch embedding from database using SQLAlchemy
â”œâ”€â”€ Verify user owns the embedding
â”œâ”€â”€ Delete embedding from vector database service
â””â”€â”€ Return: 204 No Content
```

---

## 13. Error Handling Data Flows

**ğŸ‘¤ Primary Backend Developer Responsibility (with AI/ML Engineer input on AI-specific errors)**

### 13.1 Validation Error Flow

```
Request â†’ Pydantic Validation (Automatic)
â”œâ”€â”€ Automatic field validation with Pydantic models
â”‚   â”œâ”€â”€ Validate field presence and types
â”‚   â”œâ”€â”€ Check field constraints (min/max length, etc.)
â”‚   â”œâ”€â”€ Validate email format with EmailStr
â”‚   â””â”€â”€ Validate UUID format automatically
â”œâ”€â”€ Custom validators with Pydantic @validator decorators
â”‚   â”œâ”€â”€ Business logic validation
â”‚   â”œâ”€â”€ Cross-field validation
â”‚   â””â”€â”€ Format-specific validation
â”œâ”€â”€ If validation fails
â”‚   â”œâ”€â”€ FastAPI automatically formats validation errors
â”‚   â”œâ”€â”€ Return 422 Unprocessable Entity
â”‚   â””â”€â”€ Include detailed field-level errors
â””â”€â”€ If validation passes: Continue to endpoint handler
```

**Error Response (FastAPI Standard):**
```python
# FastAPI automatically generates this format
{
  "detail": [
    {
      "loc": ["body", "email"],
      "msg": "value is not a valid email address",
      "type": "value_error.email"
    },
    {
      "loc": ["body", "password"],
      "msg": "ensure this value has at least 8 characters",
      "type": "value_error.any_str.min_length"
    }
  ]
}
```

### 13.2 Authentication Error Flow

```
Request â†’ JWT Authentication (FastAPI Dependency)
â”œâ”€â”€ Extract JWT token from Authorization header
â”‚   â”œâ”€â”€ Check header format: "Bearer <token>"
â”‚   â””â”€â”€ Extract token value
â”œâ”€â”€ Verify token using python-jose
â”‚   â”œâ”€â”€ Decode JWT token with secret key
â”‚   â”œâ”€â”€ Verify signature and algorithm
â”‚   â”œâ”€â”€ Check expiration time
â”‚   â””â”€â”€ Validate token structure
â”œâ”€â”€ Check token blacklist (if implemented)
â”œâ”€â”€ If invalid token
â”‚   â”œâ”€â”€ Raise HTTPException(status_code=401)
â”‚   â””â”€â”€ FastAPI returns standardized error
â””â”€â”€ If valid token: Inject user into endpoint function
```

**Error Response (Custom HTTPException):**
```python
raise HTTPException(
    status_code=401,
    detail="Could not validate credentials",
    headers={"WWW-Authenticate": "Bearer"},
)
```

### 13.3 Authorization Error Flow

```
Request â†’ Authorization Check (Custom Dependency)
â”œâ”€â”€ Get current user from JWT dependency
â”œâ”€â”€ Fetch resource from database using SQLAlchemy
â”œâ”€â”€ Verify user ownership using Python logic
â”‚   â”œâ”€â”€ Check owner_id field matches user.id
â”‚   â”œâ”€â”€ Verify parent folder ownership through relationships
â”‚   â””â”€â”€ Validate access permissions
â”œâ”€â”€ If not authorized
â”‚   â”œâ”€â”€ Raise HTTPException(status_code=403)
â”‚   â””â”€â”€ Return standardized error response
â””â”€â”€ If authorized: Continue to endpoint handler
```

**Error Response (Custom HTTPException):**
```python
raise HTTPException(
    status_code=403,
    detail="Not enough permissions"
)
```

### 13.4 Rate Limiting Flow

```
Request â†’ Rate Limit Middleware (slowapi)
â”œâ”€â”€ Extract user identifier (user ID or IP address)
â”œâ”€â”€ Check user's request count using Redis or memory store
â”‚   â”œâ”€â”€ Query rate limit store for current time window
â”‚   â”œâ”€â”€ Count requests in last minute/hour
â”‚   â””â”€â”€ Check against configured limit
â”œâ”€â”€ If limit exceeded
â”‚   â”œâ”€â”€ Raise HTTPException(status_code=429)
â”‚   â”œâ”€â”€ Include Retry-After header
â”‚   â””â”€â”€ Return error with retry information
â””â”€â”€ If within limit: Continue to endpoint handler
```

**Error Response (slowapi):**
```python
{
  "detail": "Rate limit exceeded: 100 per 1 minute"
}
```

### 13.5 File Upload Error Flow

```
Request â†’ File Upload Validation (FastAPI UploadFile)
â”œâ”€â”€ Validate file size using custom dependency
â”‚   â”œâ”€â”€ Check file.size against maximum (25MB)
â”‚   â””â”€â”€ If too large: Raise HTTPException(413)
â”œâ”€â”€ Validate file type using custom validation
â”‚   â”œâ”€â”€ Check MIME type from file.content_type
â”‚   â”œâ”€â”€ Verify file extension
â”‚   â””â”€â”€ If unsupported: Raise HTTPException(415)
â”œâ”€â”€ Check storage quota (if implemented)
â”‚   â”œâ”€â”€ Calculate user's total storage using SQLAlchemy
â”‚   â”œâ”€â”€ Compare with limit
â”‚   â””â”€â”€ If exceeded: Raise HTTPException(413)
â””â”€â”€ If all validations pass: Process file upload
```

**Error Response (Custom HTTPException):**
```python
raise HTTPException(
    status_code=413,
    detail={
        "message": "File too large",
        "max_size": 26214400,  # 25MB in bytes
        "actual_size": file.size
    }
)
```

---

**Technology Stack Summary:**
- **Framework**: FastAPI with Python 3.9+
- **Database**: 
  - **PostgreSQL**: Primary database for structured data (SQLAlchemy ORM)
  - **pgvector**: PostgreSQL extension for vector embeddings storage and similarity search
- **Authentication**: python-jose for JWT, passlib for password hashing
- **Validation**: Pydantic models with automatic validation
- **AI Integration**: 
  - **OpenAI API**: GPT models for chat, quiz generation, notes, open-ended questions
  - **OpenAI Embeddings**: text-embedding-ada-002 for vector embeddings
  - **httpx**: Async HTTP client for AI service calls
- **Vector Operations**:
  - **pgvector**: Cosine similarity search with vector indexes
  - **Text Chunking**: Overlap-based content segmentation
  - **Semantic Search**: Vector similarity for content retrieval
- **File Processing**: 
  - **PyPDF2/pdfplumber**: PDF text extraction
  - **python-docx**: DOCX text extraction
  - **aiofiles**: Async file operations
- **New Features**:
  - **Open-ended Questions**: AI-powered long-form question generation and grading
  - **Flashcards**: Intelligent flashcard generation from content
  - **Study Guides**: Personalized study schedule creation and tracking
- **Testing**: pytest with httpx TestClient
- **Async Support**: Full async/await throughout the application

---

## 14. Integration & Deployment Coordination

**ğŸ¤ Joint Responsibility: Both developers must coordinate for successful integration**

### 14.1 Mock Development Strategy

**Backend Developer Mock Implementations:**
```python
# Mock AI services for independent development
class MockAIService:
    async def generate_quiz(self, file_ids: List[UUID], params: QuizParams) -> QuizData:
        return QuizData(questions=[{"id": "mock", "prompt": "Mock question"}])
    
    async def search_similar_content(self, query: str, user_id: UUID) -> List[SearchResult]:
        return [SearchResult(content="Mock content", similarity=0.9)]
```

**AI/ML Engineer Test Endpoints:**
```python
# Test endpoints for AI functionality
@router.post("/test/embeddings")
async def test_embedding_pipeline(text: str):
    # Test embedding generation and storage
    
@router.post("/test/similarity") 
async def test_similarity_search(query: str):
    # Test vector search functionality
```

### 14.2 Integration Testing Checkpoints

**Phase 1: Schema Validation**
- âœ… Database schema matches both teams' requirements
- âœ… API contracts agreed upon and documented
- âœ… Mock services return expected data formats

**Phase 2: Basic Integration**
- âœ… File upload triggers embedding generation
- âœ… Chat messages call AI service correctly
- âœ… Vector search returns formatted results

**Phase 3: End-to-End Testing**
- âœ… Complete user workflows function
- âœ… Error handling works across service boundaries
- âœ… Performance meets requirements

### 14.3 Production Deployment Checklist

**Backend Developer:**
- [ ] API endpoints deployed and health-checked
- [ ] Database migrations applied
- [ ] Authentication/authorization working
- [ ] File upload/storage configured

**AI/ML Engineer:**
- [ ] OpenAI API keys configured
- [ ] Vector embeddings pipeline operational
- [ ] pgvector indexes optimized
- [ ] AI service error handling tested

**Joint:**
- [ ] Integration tests passing
- [ ] Performance benchmarks met
- [ ] Monitoring and logging configured
- [ ] Documentation updated

---

This comprehensive data flow documentation provides a complete guide for implementing the EdutechHackathon backend API using Python and FastAPI with a split development team. It covers all endpoints, vector database operations, error handling, AI integration patterns, and coordination strategies for successful parallel development of enhanced educational features. 