# Backend Data Flows Documentation

This document outlines all the data flows for the EdutechHackathon backend API built with Python and FastAPI, organized by functionality and resource type.

## Table of Contents

1. [Team Structure & Responsibility Split](#1-team-structure--responsibility-split)
2. [Authentication Data Flows](#2-authentication-data-flows)
3. [Folder Management Data Flows](#3-folder-management-data-flows)
4. [Space Management Data Flows](#4-space-management-data-flows)
5. [opip](#5-file-management-data-flows)
6. [Chat Data Flows](#6-chat-data-flows)
7. [Quiz Data Flows](#7-quiz-data-flows)
8. [Notes Data Flows](#8-notes-data-flows)
9. [Open-ended Questions Data Flows](#9-open-ended-questions-data-flows)
10. [Flashcards Data Flows](#10-flashcards-data-flows)
11. [Study Guides Data Flows](#11-study-guides-data-flows)
12. [Vector Database Operations](#12-vector-database-operations)
13. [Error Handling Data Flows](#13-error-handling-data-flows)
14. [Integration & Deployment Coordination](#14-integration--deployment-coordination)

---

## 1. Team Structure & Responsibility Split

### 1.1 Development Team Overview

The backend development is split between two specialized roles to enable parallel development and leverage domain expertise:

- **Backend Developer**: Core API, database, authentication, business logic
- **AI/ML Engineer**: AI integration, vector embeddings, semantic search, ML operations

### 1.2 Responsibility Matrix

| Area | Backend Developer | AI/ML Engineer | Integration Point |
|------|------------------|----------------|-------------------|
| **Core Infrastructure** | | | |
| FastAPI setup, routing | ✅ | | |
| SQLAlchemy models | ✅ | | Shared schema definitions |
| Authentication/Authorization | ✅ | | User context passing |
| Database migrations | ✅ | | Vector table schema |
| **Data Management** | | | |
| CRUD operations | ✅ | | |
| File upload/storage | ✅ | | File path/metadata passing |
| Text extraction | ✅ | | Raw text handoff |
| Vector table setup | ✅ | ✅ | Schema coordination |
| **AI/ML Operations** | | | |
| OpenAI API integration | | ✅ | |
| Embedding generation | | ✅ | File content input |
| Vector similarity search | | ✅ | Search results output |
| Prompt engineering | | ✅ | |
| **Business Logic** | | | |
| User/folder/space management | ✅ | | |
| File processing pipeline | ✅ | ✅ | Async task handoff |
| Chat session management | ✅ | ✅ | Message storage + AI response |
| Quiz/notes/flashcard CRUD | ✅ | ✅ | Content generation |
| **Integration & Testing** | | | |
| API endpoint testing | ✅ | | |
| AI service testing | | ✅ | |
| Integration testing | ✅ | ✅ | Joint responsibility |

### 1.3 Integration Contracts

**Key Interface Points:**
```python
# Backend → AI Service Calls
async def generate_quiz(file_ids: List[UUID], params: QuizParams) -> QuizData
async def generate_embeddings(text: str, file_id: UUID) -> List[UUID]
async def search_similar_content(query: str, user_id: UUID) -> List[SearchResult]
async def grade_open_ended_answer(answer: str, rubric: Rubric) -> GradeResult

# AI → Backend Callbacks
async def store_embeddings(embeddings: List[EmbeddingData]) -> None
async def update_file_processing_status(file_id: UUID, status: ProcessingStatus) -> None
```

### 1.4 Parallel Development Strategy

**Phase 1: Foundation (Week 1)**
- Backend: Core API endpoints, auth, database models
- AI: Service interfaces, mock implementations, vector schema design

**Phase 2: Core Features (Week 2-3)**
- Backend: File management, chat sessions, CRUD operations
- AI: Embedding pipeline, similarity search, OpenAI integration

**Phase 3: Advanced Features (Week 4-5)**
- Backend: Quiz/notes/flashcard endpoints with AI integration
- AI: Advanced prompting, grading systems, performance optimization

**Phase 4: Integration & Testing (Week 6)**
- Joint: End-to-end testing, performance tuning, deployment

### 1.5 Communication Protocols

**Daily Sync Points:**
- Morning standup: Blocker identification and resolution
- Integration checkpoints: API contract validation
- Code reviews: Cross-domain understanding

**Shared Resources:**
- API documentation (auto-generated by FastAPI)
- Database schema definitions
- Test data sets and mock responses
- Environment configuration

---

## 2. Authentication Data Flows

### 2.1 User Registration Flow

**👤 Backend Developer Responsibility**

```
Frontend → POST /api/v1/auth/register
├── Pydantic schema validation (email, password, name)
│   ├── Check email format with EmailStr
│   ├── Validate password strength with custom validator
│   └── Ensure name is provided and not empty
├── Check if user already exists in SQLAlchemy database
├── Hash password using passlib with bcrypt (salt rounds: 12)
├── Create user record in database
│   ├── id: UUID (generated with uuid4())
│   ├── email: str
│   ├── password_hash: str
│   ├── name: str
│   └── created_at: datetime
├── Generate JWT token
│   ├── payload: { sub: user_id, email: email }
│   ├── secret: settings.jwt_secret_key
│   └── expires_delta: timedelta(hours=24)
└── Return: { data: { user, token } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class UserRegistration(BaseModel):
    email: EmailStr
    password: str = Field(..., min_length=8)
    name: str = Field(..., min_length=1)
```

**Success Response:**
```json
{
  "data": {
    "user": {
      "id": "uuid",
      "email": "user@example.com",
      "name": "John Doe",
      "created_at": "2025-01-15T10:30:00Z"
    },
    "token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
  }
}
```

### 2.2 User Login Flow

**👤 Backend Developer Responsibility**

```
Frontend → POST /api/v1/auth/login
├── Pydantic schema validation (email, password)
├── Query user by email using SQLAlchemy
├── Verify password hash using passlib.verify()
├── Generate JWT token with python-jose
├── Update last_login_at timestamp
└── Return: { data: { user, token } }
```

**Request Body (Pydantic Schema):**
```python
class UserLogin(BaseModel):
    email: EmailStr
    password: str
```

### 2.3 User Profile Flow

```
Frontend → GET /api/v1/auth/profile
├── Extract user from JWT token (FastAPI Depends)
├── Fetch complete user data from SQLAlchemy database
└── Return: { data: { user } }
```

**Headers Required:**
```
Authorization: Bearer <jwt_token>
```

### 2.4 User Logout Flow

```
Frontend → POST /api/v1/auth/logout
├── Extract user from JWT token (FastAPI dependency)
├── Add token to blacklist (Redis or database table)
└── Return: 204 No Content
```

---

## 3. Folder Management Data Flows

**👤 Backend Developer Responsibility**

### 3.1 List Folders Flow

```
Frontend → GET /api/v1/folders?page=1&limit=20&q=biology
├── Extract user from JWT token (Depends(get_current_user))
├── Parse query parameters with FastAPI Query params
│   ├── page: int = Query(1, ge=1)
│   ├── limit: int = Query(20, ge=1, le=100)
│   └── q: Optional[str] = Query(None)
├── Query database using SQLAlchemy
│   ├── WHERE owner_id = user.id
│   ├── AND deleted_at IS NULL
│   └── AND title.ilike(f'%{q}%') (if search term provided)
├── Apply pagination with SQLAlchemy offset/limit
├── Count total records for pagination metadata
└── Return: { data: [folders], meta: { page, limit, total } }
```

**Success Response (Pydantic Schema):**
```python
class FolderListResponse(BaseModel):
    data: List[FolderResponse]
    meta: PaginationMeta

class FolderResponse(BaseModel):
    id: str
    owner_id: str
    title: str
    description: Optional[str]
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 3.2 Create Folder Flow

```
Frontend → POST /api/v1/folders
├── Extract user from JWT token (Depends(get_current_user))
├── Validate request body with Pydantic
│   ├── title: str (required, validated)
│   └── description: Optional[str]
├── Create folder record using SQLAlchemy
│   ├── id: UUID (generated)
│   ├── owner_id: user.id
│   ├── title: str
│   ├── description: Optional[str]
│   └── created_at: datetime.utcnow()
└── Return: { data: { folder } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class FolderCreate(BaseModel):
    title: str = Field(..., min_length=1, max_length=255)
    description: Optional[str] = Field(None, max_length=1000)
```

### 3.3 Get Folder Flow

```
Frontend → GET /api/v1/folders/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Validate folder ID format (UUID validation automatic with Path param)
├── Fetch folder from database using SQLAlchemy
├── Verify user owns the folder (owner_id == user.id)
└── Return: { data: { folder } }
```

### 3.4 Update Folder Flow

```
Frontend → PATCH /api/v1/folders/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch folder from database using SQLAlchemy
├── Verify user owns the folder
├── Validate request body with Pydantic (partial update)
├── Update folder fields in database
└── Return: { data: { folder } }
```

**Request Body (Pydantic Schema):**
```python
class FolderUpdate(BaseModel):
    title: Optional[str] = Field(None, min_length=1, max_length=255)
    description: Optional[str] = Field(None, max_length=1000)
```

### 3.5 Delete Folder Flow

```
Frontend → DELETE /api/v1/folders/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch folder from database using SQLAlchemy
├── Verify user owns the folder
├── Perform cascade delete using SQLAlchemy relationships
│   ├── Delete all spaces in folder (CASCADE)
│   ├── Delete all files in folder (CASCADE)
│   ├── Delete all messages in spaces (CASCADE)
│   ├── Delete all quizzes in spaces (CASCADE)
│   └── Delete all notes in spaces (CASCADE)
└── Return: 204 No Content
```

---

## 4. Space Management Data Flows

**👤 Backend Developer Responsibility**

### 4.1 List Spaces in Folder Flow

```
Frontend → GET /api/v1/folders/{folderId}/spaces?type=chat&page=1
├── Extract user from JWT token (Depends(get_current_user))
├── Validate folder ID format (UUID automatic validation)
├── Verify user owns the folder
├── Parse query parameters with FastAPI
│   ├── type: Optional[SpaceType] = Query(None)
│   ├── page: int = Query(1, ge=1)
│   └── limit: int = Query(20, ge=1, le=100)
├── Query spaces using SQLAlchemy with relationships
│   ├── WHERE folder_id = folder_id
│   ├── AND deleted_at IS NULL
│   └── AND type = type (if filter provided)
├── Apply pagination with SQLAlchemy
└── Return: { data: [spaces], meta: { page, limit, total } }
```

**Success Response (Pydantic Schema):**
```python
class SpaceType(str, Enum):
    chat = "chat"
    quiz = "quiz"
    notes = "notes"
    openended = "openended"
    flashcards = "flashcards"
    studyguide = "studyguide"

class SpaceResponse(BaseModel):
    id: str
    folder_id: str
    type: SpaceType
    title: str
    settings: Dict[str, Any]
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 4.2 Create Space Flow

```
Frontend → POST /api/v1/folders/{folderId}/spaces
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the folder
├── Validate request body with Pydantic
│   ├── type: SpaceType (enum validation)
│   ├── title: str (required)
│   └── settings: Optional[Dict] = {}
├── Create space record using SQLAlchemy
│   ├── id: UUID (generated)
│   ├── folder_id: folder_id
│   ├── type: SpaceType
│   ├── title: str
│   ├── settings: dict (JSON field)
│   └── created_at: datetime.utcnow()
└── Return: { data: { space } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class SpaceCreate(BaseModel):
    type: SpaceType
    title: str = Field(..., min_length=1, max_length=255)
    settings: Optional[Dict[str, Any]] = Field(default_factory=dict)
```

### 4.3 Get Space Flow

```
Frontend → GET /api/v1/spaces/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch space from database using SQLAlchemy with joins
├── Verify user owns the parent folder (through relationship)
└── Return: { data: { space } }
```

### 4.4 Update Space Flow

```
Frontend → PATCH /api/v1/spaces/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch space from database using SQLAlchemy
├── Verify user owns the parent folder
├── Validate request body with Pydantic (partial update)
├── Update space fields using SQLAlchemy
└── Return: { data: { space } }
```

### 4.5 Delete Space Flow

```
Frontend → DELETE /api/v1/spaces/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch space from database using SQLAlchemy
├── Verify user owns the parent folder
├── Perform cascade delete using SQLAlchemy relationships
│   ├── Delete all messages in space (CASCADE)
│   ├── Delete all quizzes in space (CASCADE)
│   └── Delete all notes in space (CASCADE)
└── Return: 204 No Content
```

---

## 5. File Management Data Flows

**🤝 Shared Responsibility: Backend Developer (upload/storage) + AI/ML Engineer (embedding generation)**

### 5.1 Upload Files Flow

```
Frontend → POST /api/v1/files/upload (multipart/form-data)
├── Extract user from JWT token (Depends(get_current_user))
├── Validate multipart form data with FastAPI UploadFile
│   ├── folder_id: str (Form field, UUID validation)
│   └── files: List[UploadFile] (FastAPI file handling)
├── Verify user owns the folder
├── Validate each file with custom validators
│   ├── Check file size (max 25MB)
│   ├── Validate MIME type (PDF, DOCX, TXT, MD)
│   └── Scan filename for security
├── Save files to storage using async file operations
│   ├── Generate unique filename with uuid4()
│   ├── Save to local storage or cloud (async)
│   └── Get file path for database
├── Extract text content (async background task)
│   ├── PDF: Use PyPDF2 or pdfplumber
│   ├── DOCX: Use python-docx
│   ├── TXT/MD: Read directly with aiofiles
│   └── Store extracted text in PostgreSQL database
├── 🔄 **HANDOFF TO AI/ML ENGINEER** - Generate vector embeddings (async background task)
│   ├── Split text into chunks (1000 chars with 200 char overlap)
│   ├── Generate embeddings using OpenAI Embeddings API
│   ├── Store embeddings in pgvector with metadata
│   │   ├── embedding: vector (1536 dimensions for text-embedding-ada-002)
│   │   ├── file_id: UUID (links to main file record)
│   │   ├── chunk_text: str (original text chunk)
│   │   ├── chunk_index: int (position in file)
│   │   └── created_at: datetime
│   └── Link vector IDs to file record in PostgreSQL
├── Create file records using SQLAlchemy in PostgreSQL
│   ├── id: UUID (generated)
│   ├── folder_id: folder_id
│   ├── name: original filename
│   ├── mime_type: detected MIME type
│   ├── size: file size in bytes
│   ├── path: storage path
│   ├── text_content: extracted text
│   ├── vector_ids: List[UUID] (references to pgvector embeddings)
│   └── created_at: datetime.utcnow()
└── Return: { data: [files] } (201 Created)
```

**Request Body (FastAPI Form):**
```python
@router.post("/upload")
async def upload_files(
    folder_id: str = Form(...),
    files: List[UploadFile] = File(...),
    current_user: User = Depends(get_current_user)
):
    # Implementation
```

**Success Response (Pydantic Schema):**
```python
class FileResponse(BaseModel):
    id: str
    folder_id: str
    name: str
    mime_type: str
    size: int
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 5.2 List Files in Folder Flow

```
Frontend → GET /api/v1/folders/{folderId}/files?page=1
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the folder
├── Query files using SQLAlchemy
│   ├── WHERE folder_id = folder_id
│   ├── AND deleted_at IS NULL
│   └── ORDER BY created_at DESC
├── Apply pagination with SQLAlchemy
└── Return: { data: [files], meta: { page, limit, total } }
```

### 5.3 Get File Metadata Flow

```
Frontend → GET /api/v1/files/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch file from database using SQLAlchemy
├── Verify user owns the parent folder (through relationship)
└── Return: { data: { file } }
```

### 5.4 Get File Content Flow

```
Frontend → GET /api/v1/files/{id}/content
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch file from database using SQLAlchemy
├── Verify user owns the parent folder
├── Check if text extraction is complete
├── Return file content from database or storage
└── Return: { data: { content, mime_type } }
```

### 5.5 Delete File Flow

```
Frontend → DELETE /api/v1/files/{id}?force=false
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch file from database using SQLAlchemy
├── Verify user owns the parent folder
├── Check force parameter (Query param)
├── If force=true: Hard delete
│   ├── Remove file from storage using async operations
│   └── Delete database record
├── If force=false: Soft delete
│   └── Set deleted_at timestamp
└── Return: 204 No Content
```

---

## 6. Chat Data Flows

**🤝 Shared Responsibility: Backend Developer (session management) + AI/ML Engineer (AI response generation)**

### 6.1 Send Message Flow

```
Frontend → POST /api/v1/spaces/{spaceId}/messages
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder (through space relationship)
├── Validate request body with Pydantic
│   ├── content: str (required, validated)
│   └── role: MessageRole = 'user' (enum validation)
├── Save user message using SQLAlchemy
│   ├── id: UUID (generated)
│   ├── space_id: space_id
│   ├── role: MessageRole.user
│   ├── content: str
│   ├── sources: List[Dict] = []
│   └── created_at: datetime.utcnow()
├── 🔄 **HANDOFF TO AI/ML ENGINEER** - Generate query embedding for similarity search
│   ├── Create embedding from user message using OpenAI Embeddings API
│   └── Prepare embedding vector for pgvector search
├── 🤖 **AI/ML ENGINEER** - Perform vector similarity search using pgvector
│   ├── Query pgvector for similar content using cosine similarity
│   ├── Filter by user's accessible files (folder ownership)
│   ├── Return top-k relevant chunks (k=5-10)
│   └── Include file metadata and chunk context
├── 🤖 **AI/ML ENGINEER** - Prepare context for AI using async operations
│   ├── Get recent messages (PostgreSQL query with limit)
│   ├── Get relevant file chunks from vector search results
│   ├── Combine chat history with relevant context
│   └── Format context for AI prompt
├── 🤖 **AI/ML ENGINEER** - Call OpenAI Service using httpx async client
│   ├── OpenAI API client with async httpx
│   │   ├── model: settings.ai_model (from config)
│   │   ├── messages: conversation history + context + system prompt
│   │   ├── max_tokens: settings.ai_max_tokens
│   │   └── temperature: settings.ai_temperature
│   ├── Process response with citations using custom logic
│   │   ├── Extract source references from response
│   │   ├── Map to file IDs, chunk indices, and pages
│   │   └── Format citations with vector similarity scores
│   └── Return structured response with enhanced citations
├── Save assistant message using SQLAlchemy in PostgreSQL
│   ├── id: UUID (generated)
│   ├── space_id: space_id
│   ├── role: MessageRole.assistant
│   ├── content: AI response
│   ├── sources: citation array (JSON field with vector metadata)
│   ├── vector_chunks_used: List[UUID] (pgvector chunk references)
│   ├── similarity_scores: List[float] (vector search scores)
│   └── created_at: datetime.utcnow()
└── Return: { data: { message } }
```

**Request Body (Pydantic Schema):**
```python
class MessageRole(str, Enum):
    user = "user"
    assistant = "assistant"

class MessageCreate(BaseModel):
    content: str = Field(..., min_length=1, max_length=10000)
    role: MessageRole = MessageRole.user
```

**Success Response (Pydantic Schema):**
```python
class MessageResponse(BaseModel):
    id: str
    space_id: str
    role: MessageRole
    content: str
    sources: List[Dict[str, Any]]
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 6.2 Stream Chat Reply Flow

```
Frontend → POST /api/v1/spaces/{spaceId}/messages?stream=true
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder
├── Save user message to database using SQLAlchemy
├── Set up Server-Sent Events using FastAPI StreamingResponse
│   ├── Set headers: Content-Type: text/event-stream
│   ├── Set headers: Cache-Control: no-cache
│   └── Set headers: Connection: keep-alive
├── Call OpenAI Service with streaming using async generator
│   ├── OpenAI API with stream=True using async httpx
│   │   ├── model: settings.ai_model
│   │   ├── stream: True
│   │   └── Same parameters as non-streaming
│   ├── Process streaming chunks with async for loop
│   │   ├── Parse OpenAI stream format
│   │   ├── Extract content deltas
│   │   └── Handle citations in final chunk
│   └── Yield chunks using async generator
│       ├── Format: "data: {json.dumps(chunk)}\n\n"
│       └── Yield each chunk immediately
├── Accumulate full response in memory
├── Save complete assistant message using SQLAlchemy
└── Yield: "data: {json.dumps({'done': True})}\n\n"
```

**FastAPI Streaming Implementation:**
```python
@router.post("/messages")
async def send_message(
    space_id: str,
    message: MessageCreate,
    stream: bool = Query(False),
    current_user: User = Depends(get_current_user)
):
    if stream:
        return StreamingResponse(
            stream_ai_response(message, space_id, current_user),
            media_type="text/event-stream"
        )
    # Regular response logic

async def stream_ai_response(message: MessageCreate, space_id: str, user: User):
    async for chunk in ai_service.stream_response(message, space_id):
        yield f"data: {json.dumps(chunk)}\n\n"
```

### 6.3 Get Chat History Flow

```
Frontend → GET /api/v1/spaces/{spaceId}/messages?page=1
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder
├── Query messages using SQLAlchemy
│   ├── WHERE space_id = space_id
│   ├── AND deleted_at IS NULL
│   └── ORDER BY created_at ASC
├── Apply pagination with SQLAlchemy
└── Return: { data: [messages], meta: { page, limit, total } }
```

### 6.4 Delete Message Flow

```
Frontend → DELETE /api/v1/messages/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch message from database using SQLAlchemy
├── Verify user owns the parent folder (through relationships)
├── Soft delete message (set deleted_at timestamp)
└── Return: 204 No Content
```

---

## 7. Quiz Data Flows

**🤝 Shared Responsibility: Backend Developer (CRUD operations) + AI/ML Engineer (content generation)**

### 7.1 Generate Quiz Flow

```
Frontend → POST /api/v1/spaces/{spaceId}/quizzes
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder
├── Validate request body with Pydantic
│   ├── title: str (required)
│   ├── file_ids: List[str] (UUID validation)
│   ├── question_count: int = 10 (default, validated)
│   ├── question_types: List[QuestionType] = ['mcq', 'tf']
│   └── difficulty: DifficultyLevel = 'medium'
├── Fetch file contents using PostgreSQL and pgvector queries
│   ├── Verify all files belong to user (ownership check)
│   ├── Get text content from PostgreSQL database
│   ├── Retrieve related vector embeddings from pgvector
│   ├── Perform content clustering using vector similarity
│   └── Select diverse content chunks for quiz generation
├── Call OpenAI Service for quiz generation using async httpx
│   ├── OpenAI API with custom prompt for quiz generation
│   │   ├── model: settings.ai_model
│   │   ├── prompt: "Generate quiz questions from content..."
│   │   ├── parameters: question_count, types, difficulty
│   │   └── response_format: structured JSON
│   ├── Generate questions from file content using AI
│   │   ├── Multiple choice questions with 4 options
│   │   ├── True/false questions with explanations
│   │   ├── Short answer questions with sample answers
│   │   └── Ensure answer accuracy and relevance
│   ├── Parse and validate AI response structure
│   └── Return structured quiz object
├── Save quiz using SQLAlchemy
│   ├── id: UUID (generated)
│   ├── space_id: space_id
│   ├── title: str
│   ├── questions: List[Dict] (JSON field with questions)
│   ├── file_ids: List[str] (JSON field with source files)
│   └── created_at: datetime.utcnow()
└── Return: { data: { quiz } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class QuestionType(str, Enum):
    mcq = "mcq"
    tf = "tf"
    short_answer = "short_answer"

class DifficultyLevel(str, Enum):
    easy = "easy"
    medium = "medium"
    hard = "hard"

class QuizCreate(BaseModel):
    title: str = Field(..., min_length=1, max_length=255)
    file_ids: List[str] = Field(..., min_items=1)
    question_count: int = Field(10, ge=1, le=50)
    question_types: List[QuestionType] = Field(default=[QuestionType.mcq, QuestionType.tf])
    difficulty: DifficultyLevel = DifficultyLevel.medium
```

**Success Response (Pydantic Schema):**
```python
class QuizResponse(BaseModel):
    id: str
    space_id: str
    title: str
    questions: List[Dict[str, Any]]
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 7.2 List Quizzes Flow

```
Frontend → GET /api/v1/spaces/{spaceId}/quizzes?page=1
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder
├── Query quizzes using SQLAlchemy
│   ├── WHERE space_id = space_id
│   ├── AND deleted_at IS NULL
│   └── ORDER BY created_at DESC
├── Apply pagination with SQLAlchemy
└── Return: { data: [quizzes], meta: { page, limit, total } }
```

### 7.3 Get Quiz Detail Flow

```
Frontend → GET /api/v1/quizzes/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch quiz from database using SQLAlchemy
├── Verify user owns the parent folder (through relationships)
└── Return: { data: { quiz } }
```

### 7.4 Submit Quiz Answers Flow

```
Frontend → POST /api/v1/quizzes/{id}/submit
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch quiz from database using SQLAlchemy
├── Verify user owns the parent folder
├── Validate submitted answers with Pydantic
│   ├── Check answer format for each question type
│   ├── Ensure all required questions answered
│   └── Validate answer choices exist for MCQ
├── Grade answers using AI service and custom logic
│   ├── Grade MCQ and T&F automatically with Python logic
│   ├── Grade short answers using OpenAI API
│   │   ├── Call OpenAI API for each short answer
│   │   ├── Compare with expected answer using AI
│   │   └── Assign partial credit based on AI evaluation
│   ├── Calculate total score using Python calculations
│   ├── Generate feedback for each question
│   └── Return grading results
├── Save submission using SQLAlchemy
│   ├── id: UUID (generated)
│   ├── quiz_id: quiz_id
│   ├── user_id: user.id
│   ├── answers: List[Dict] (JSON field)
│   ├── score: float
│   ├── feedback: List[Dict] (JSON field)
│   └── submitted_at: datetime.utcnow()
└── Return: { data: { score, feedback, answers } }
```

**Request Body (Pydantic Schema):**
```python
class QuizAnswer(BaseModel):
    question_id: str
    answer: Union[str, int]  # String for text, int for choice index

class QuizSubmission(BaseModel):
    answers: List[QuizAnswer] = Field(..., min_items=1)
```

**Success Response (Pydantic Schema):**
```python
class QuizSubmissionResponse(BaseModel):
    score: float
    total_questions: int
    feedback: List[Dict[str, Any]]
    submitted_at: datetime
```

### 7.5 Delete Quiz Flow

```
Frontend → DELETE /api/v1/quizzes/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch quiz from database using SQLAlchemy
├── Verify user owns the parent folder
├── Delete quiz and all submissions using SQLAlchemy CASCADE
└── Return: 204 No Content
```

---

## 8. Notes Data Flows

### 8.1 Generate Notes Flow

```
Frontend → POST /api/v1/spaces/{spaceId}/notes
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder
├── Validate request body with Pydantic
│   ├── file_ids: List[str] (UUID validation, required)
│   └── format: NotesFormat = 'markdown' (enum validation)
├── Fetch file contents using PostgreSQL and pgvector queries
│   ├── Verify all files belong to user (ownership validation)
│   ├── Get text content from PostgreSQL database
│   ├── Retrieve related vector embeddings from pgvector
│   ├── Perform semantic clustering of content using vector similarity
│   └── Organize content chunks by topic for structured notes
├── Call OpenAI Service for notes generation using async httpx
│   ├── OpenAI API with custom prompt for note generation
│   │   ├── model: settings.ai_model
│   │   ├── prompt: "Generate summary notes from content..."
│   │   ├── parameters: format preference
│   │   └── response_format: structured text
│   ├── Generate notes from file content using AI
│   │   ├── Extract key concepts and important information
│   │   ├── Organize information logically by topics
│   │   ├── Include important details and examples
│   │   └── Format according to user preference
│   ├── Format output as markdown or bullet points
│   └── Return structured notes object
├── Save notes using SQLAlchemy
│   ├── id: UUID (generated)
│   ├── space_id: space_id
│   ├── format: NotesFormat
│   ├── content: str (formatted notes content)
│   ├── file_ids: List[str] (JSON field with source files)
│   └── created_at: datetime.utcnow()
└── Return: { data: { notes } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class NotesFormat(str, Enum):
    markdown = "markdown"
    bullet = "bullet"

class NotesCreate(BaseModel):
    file_ids: List[str] = Field(..., min_items=1)
    format: NotesFormat = NotesFormat.markdown
```

**Success Response (Pydantic Schema):**
```python
class NotesResponse(BaseModel):
    id: str
    space_id: str
    format: NotesFormat
    content: str
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 8.2 List Notes Flow

```
Frontend → GET /api/v1/spaces/{spaceId}/notes?page=1
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder
├── Query notes using SQLAlchemy
│   ├── WHERE space_id = space_id
│   ├── AND deleted_at IS NULL
│   └── ORDER BY created_at DESC
├── Apply pagination with SQLAlchemy
└── Return: { data: [notes], meta: { page, limit, total } }
```

### 8.3 Get Note Flow

```
Frontend → GET /api/v1/notes/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch note from database using SQLAlchemy
├── Verify user owns the parent folder (through relationships)
└── Return: { data: { note } }
```

### 8.4 Update Note Flow

```
Frontend → PATCH /api/v1/notes/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch note from database using SQLAlchemy
├── Verify user owns the parent folder
├── Validate request body with Pydantic (partial update)
├── Update note content using SQLAlchemy
├── Update updated_at timestamp
└── Return: { data: { note } }
```

**Request Body (Pydantic Schema):**
```python
class NotesUpdate(BaseModel):
    content: Optional[str] = Field(None, min_length=1)
```

### 8.5 Delete Note Flow

```
Frontend → DELETE /api/v1/notes/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch note from database using SQLAlchemy
├── Verify user owns the parent folder
└── Return: 204 No Content
```

---

## 9. Open-ended Questions Data Flows

**🤝 Shared Responsibility: Backend Developer (CRUD operations) + AI/ML Engineer (question generation & grading)**

### 9.1 Generate Open-ended Question Flow

```
Frontend → POST /api/v1/spaces/{spaceId}/open-ended-questions
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder
├── Validate request body with Pydantic
│   ├── title: str (required)
│   ├── file_ids: List[str] (UUID validation, required)
│   ├── question_count: int = 10 (default, validated)
│   ├── question_types: List[QuestionType] = ['short_answer']
│   └── difficulty: DifficultyLevel = 'medium'
├── Fetch file contents using PostgreSQL and pgvector queries
│   ├── Verify all files belong to user (ownership check)
│   ├── Get text content from PostgreSQL database
│   ├── Retrieve related vector embeddings from pgvector
│   ├── Perform semantic analysis using vector similarity
│   └── Select diverse content chunks for question generation
├── Call OpenAI Service for open-ended question generation using async httpx
│   ├── OpenAI API with custom prompt for open-ended question generation
│   │   ├── model: settings.ai_model
│   │   ├── prompt: "Generate open-ended questions from content..."
│   │   ├── parameters: question_count, types, difficulty
│   │   └── response_format: structured JSON
│   ├── Generate questions from file content using AI
│   │   ├── Short answer questions with sample answers
│   │   ├── Ensure answer accuracy and relevance
│   │   └── Format according to user preference
│   ├── Parse and validate AI response structure
│   └── Return structured question object
├── Save open-ended question using SQLAlchemy
│   ├── id: UUID (generated)
│   ├── space_id: space_id
│   ├── title: str
│   ├── questions: List[Dict] (JSON field with questions)
│   ├── file_ids: List[str] (JSON field with source files)
│   └── created_at: datetime.utcnow()
└── Return: { data: { open_ended_question } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class OpenEndedQuestionType(str, Enum):
    short_answer = "short_answer"

class OpenEndedDifficultyLevel(str, Enum):
    easy = "easy"
    medium = "medium"
    hard = "hard"

class OpenEndedQuestionCreate(BaseModel):
    title: str = Field(..., min_length=1, max_length=255)
    file_ids: List[str] = Field(..., min_items=1)
    question_count: int = Field(10, ge=1, le=50)
    question_types: List[OpenEndedQuestionType] = Field(default=[OpenEndedQuestionType.short_answer])
    difficulty: OpenEndedDifficultyLevel = OpenEndedDifficultyLevel.medium
```

**Success Response (Pydantic Schema):**
```python
class OpenEndedQuestionResponse(BaseModel):
    id: str
    space_id: str
    title: str
    questions: List[Dict[str, Any]]
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 8.2 List Open-ended Questions Flow

```
Frontend → GET /api/v1/spaces/{spaceId}/open-ended-questions?page=1
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder
├── Query open-ended questions using SQLAlchemy
│   ├── WHERE space_id = space_id
│   ├── AND deleted_at IS NULL
│   └── ORDER BY created_at DESC
├── Apply pagination with SQLAlchemy
└── Return: { data: [open_ended_questions], meta: { page, limit, total } }
```

### 8.3 Get Open-ended Question Detail Flow

```
Frontend → GET /api/v1/open-ended-questions/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch open-ended question from database using SQLAlchemy
├── Verify user owns the parent folder (through relationships)
└── Return: { data: { open_ended_question } }
```

### 8.4 Submit Open-ended Question Answers Flow

```
Frontend → POST /api/v1/open-ended-questions/{id}/submit
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch open-ended question from database using SQLAlchemy
├── Verify user owns the parent folder
├── Validate submitted answers with Pydantic
│   ├── Check answer format for each question type
│   ├── Ensure all required questions answered
│   └── Validate answer choices exist for MCQ
├── Grade answers using AI service and custom logic
│   ├── Grade MCQ and T&F automatically with Python logic
│   ├── Grade short answers using OpenAI API
│   │   ├── Call OpenAI API for each short answer
│   │   ├── Compare with expected answer using AI
│   │   └── Assign partial credit based on AI evaluation
│   ├── Calculate total score using Python calculations
│   ├── Generate feedback for each question
│   └── Return grading results
├── Save submission using SQLAlchemy
│   ├── id: UUID (generated)
│   ├── open_ended_question_id: open_ended_question_id
│   ├── user_id: user.id
│   ├── answers: List[Dict] (JSON field)
│   ├── score: float
│   ├── feedback: List[Dict] (JSON field)
│   └── submitted_at: datetime.utcnow()
└── Return: { data: { score, feedback, answers } }
```

**Request Body (Pydantic Schema):**
```python
class OpenEndedQuestionAnswer(BaseModel):
    question_id: str
    answer: str  # String for short answer

class OpenEndedQuestionSubmission(BaseModel):
    answers: List[OpenEndedQuestionAnswer] = Field(..., min_items=1)
```

**Success Response (Pydantic Schema):**
```python
class OpenEndedQuestionSubmissionResponse(BaseModel):
    score: float
    total_questions: int
    feedback: List[Dict[str, Any]]
    submitted_at: datetime
```

### 8.5 Delete Open-ended Question Flow

```
Frontend → DELETE /api/v1/open-ended-questions/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch open-ended question from database using SQLAlchemy
├── Verify user owns the parent folder
├── Delete open-ended question and all submissions using SQLAlchemy CASCADE
└── Return: 204 No Content
```

---

## 10. Flashcards Data Flows

### 10.1 Generate Flashcards Flow

```
Frontend → POST /api/v1/spaces/{spaceId}/flashcards
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder
├── Validate request body with Pydantic
│   ├── title: str (required)
│   ├── file_ids: List[str] (UUID validation, required)
│   ├── card_count: int = 20 (default, validated)
│   ├── card_types: List[CardType] = ['mcq', 'tf']
│   └── difficulty: DifficultyLevel = 'medium'
├── Fetch file contents using PostgreSQL and pgvector queries
│   ├── Verify all files belong to user (ownership check)
│   ├── Get text content from PostgreSQL database
│   ├── Retrieve related vector embeddings from pgvector
│   ├── Identify key terms and concepts using vector clustering
│   └── Select optimal content chunks for flashcard generation
├── Call OpenAI Service for flashcard generation using async httpx
│   ├── OpenAI API with custom prompt for flashcard generation
│   │   ├── model: settings.ai_model
│   │   ├── prompt: "Generate flashcards from content..."
│   │   ├── parameters: card_count, types, difficulty
│   │   └── response_format: structured JSON
│   ├── Generate flashcards from file content using AI
│   │   ├── Multiple choice questions with 4 options
│   │   ├── True/false questions with explanations
│   │   ├── Short answer questions with sample answers
│   │   └── Ensure answer accuracy and relevance
│   ├── Parse and validate AI response structure
│   └── Return structured flashcard object
├── Save flashcard using SQLAlchemy
│   ├── id: UUID (generated)
│   ├── space_id: space_id
│   ├── title: str
│   ├── flashcards: List[Dict] (JSON field with flashcards)
│   ├── file_ids: List[str] (JSON field with source files)
│   └── created_at: datetime.utcnow()
└── Return: { data: { flashcard } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class CardType(str, Enum):
    mcq = "mcq"
    tf = "tf"
    short_answer = "short_answer"

class FlashcardDifficultyLevel(str, Enum):
    easy = "easy"
    medium = "medium"
    hard = "hard"

class FlashcardCreate(BaseModel):
    title: str = Field(..., min_length=1, max_length=255)
    file_ids: List[str] = Field(..., min_items=1)
    card_count: int = Field(20, ge=1, le=100)
    card_types: List[CardType] = Field(default=[CardType.mcq, CardType.tf])
    difficulty: FlashcardDifficultyLevel = FlashcardDifficultyLevel.medium
```

**Success Response (Pydantic Schema):**
```python
class FlashcardResponse(BaseModel):
    id: str
    space_id: str
    title: str
    flashcards: List[Dict[str, Any]]
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 10.2 List Flashcards Flow

```
Frontend → GET /api/v1/spaces/{spaceId}/flashcards?page=1
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder
├── Query flashcards using SQLAlchemy
│   ├── WHERE space_id = space_id
│   ├── AND deleted_at IS NULL
│   └── ORDER BY created_at DESC
├── Apply pagination with SQLAlchemy
└── Return: { data: [flashcards], meta: { page, limit, total } }
```

### 10.3 Get Flashcard Detail Flow

```
Frontend → GET /api/v1/flashcards/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch flashcard from database using SQLAlchemy
├── Verify user owns the parent folder (through relationships)
└── Return: { data: { flashcard } }
```

### 10.4 Submit Flashcard Answers Flow

```
Frontend → POST /api/v1/flashcards/{id}/submit
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch flashcard from database using SQLAlchemy
├── Verify user owns the parent folder
├── Validate submitted answers with Pydantic
│   ├── Check answer format for each question type
│   ├── Ensure all required questions answered
│   └── Validate answer choices exist for MCQ
├── Grade answers using AI service and custom logic
│   ├── Grade MCQ and T&F automatically with Python logic
│   ├── Grade short answers using OpenAI API
│   │   ├── Call OpenAI API for each short answer
│   │   ├── Compare with expected answer using AI
│   │   └── Assign partial credit based on AI evaluation
│   ├── Calculate total score using Python calculations
│   ├── Generate feedback for each question
│   └── Return grading results
├── Save submission using SQLAlchemy
│   ├── id: UUID (generated)
│   ├── flashcard_id: flashcard_id
│   ├── user_id: user.id
│   ├── answers: List[Dict] (JSON field)
│   ├── score: float
│   ├── feedback: List[Dict] (JSON field)
│   └── submitted_at: datetime.utcnow()
└── Return: { data: { score, feedback, answers } }
```

**Request Body (Pydantic Schema):**
```python
class FlashcardAnswer(BaseModel):
    question_id: str
    answer: Union[str, int]  # String for text, int for choice index

class FlashcardSubmission(BaseModel):
    answers: List[FlashcardAnswer] = Field(..., min_items=1)
```

**Success Response (Pydantic Schema):**
```python
class FlashcardSubmissionResponse(BaseModel):
    score: float
    total_questions: int
    feedback: List[Dict[str, Any]]
    submitted_at: datetime
```

### 10.5 Delete Flashcard Flow

```
Frontend → DELETE /api/v1/flashcards/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch flashcard from database using SQLAlchemy
├── Verify user owns the parent folder
├── Delete flashcard and all submissions using SQLAlchemy CASCADE
└── Return: 204 No Content
```

---

## 11. Study Guides Data Flows

### 11.1 Generate Study Guide Flow

```
Frontend → POST /api/v1/spaces/{spaceId}/study-guides
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder
├── Validate request body with Pydantic
│   ├── title: str (required)
│   ├── file_ids: List[str] (UUID validation, required)
│   ├── guide_type: GuideType = 'summary' (enum validation)
│   └── difficulty: DifficultyLevel = 'medium'
├── Fetch file contents using PostgreSQL and pgvector queries
│   ├── Verify all files belong to user (ownership check)
│   ├── Get text content from PostgreSQL database
│   ├── Retrieve related vector embeddings from pgvector
│   ├── Analyze content structure using vector similarity
│   ├── Create topic hierarchy from vector clusters
│   └── Organize content for structured study guide generation
├── Call OpenAI Service for study guide generation using async httpx
│   ├── OpenAI API with custom prompt for study guide generation
│   │   ├── model: settings.ai_model
│   │   ├── prompt: "Generate study guide from content..."
│   │   ├── parameters: guide_type, difficulty
│   │   └── response_format: structured text
│   ├── Generate study guide from file content using AI
│   │   ├── Extract key concepts and important information
│   │   ├── Organize information logically by topics
│   │   ├── Include important details and examples
│   │   └── Format according to user preference
│   ├── Format output as markdown or bullet points
│   └── Return structured study guide object
├── Save study guide using SQLAlchemy
│   ├── id: UUID (generated)
│   ├── space_id: space_id
│   ├── title: str
│   ├── guide_type: GuideType
│   ├── content: str (formatted guide content)
│   ├── file_ids: List[str] (JSON field with source files)
│   └── created_at: datetime.utcnow()
└── Return: { data: { study_guide } } (201 Created)
```

**Request Body (Pydantic Schema):**
```python
class GuideType(str, Enum):
    summary = "summary"
    detailed = "detailed"

class StudyGuideDifficultyLevel(str, Enum):
    easy = "easy"
    medium = "medium"
    hard = "hard"

class StudyGuideCreate(BaseModel):
    title: str = Field(..., min_length=1, max_length=255)
    file_ids: List[str] = Field(..., min_items=1)
    guide_type: GuideType = GuideType.summary
    difficulty: StudyGuideDifficultyLevel = StudyGuideDifficultyLevel.medium
```

**Success Response (Pydantic Schema):**
```python
class StudyGuideResponse(BaseModel):
    id: str
    space_id: str
    title: str
    guide_type: GuideType
    content: str
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 11.2 List Study Guides Flow

```
Frontend → GET /api/v1/spaces/{spaceId}/study-guides?page=1
├── Extract user from JWT token (Depends(get_current_user))
├── Verify user owns the parent folder
├── Query study guides using SQLAlchemy
│   ├── WHERE space_id = space_id
│   ├── AND deleted_at IS NULL
│   └── ORDER BY created_at DESC
├── Apply pagination with SQLAlchemy
└── Return: { data: [study_guides], meta: { page, limit, total } }
```

### 11.3 Get Study Guide Detail Flow

```
Frontend → GET /api/v1/study-guides/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch study guide from database using SQLAlchemy
├── Verify user owns the parent folder (through relationships)
└── Return: { data: { study_guide } }
```

### 11.4 Submit Study Guide Answers Flow

```
Frontend → POST /api/v1/study-guides/{id}/submit
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch study guide from database using SQLAlchemy
├── Verify user owns the parent folder
├── Validate submitted answers with Pydantic
│   ├── Check answer format for each question type
│   ├── Ensure all required questions answered
│   └── Validate answer choices exist for MCQ
├── Grade answers using AI service and custom logic
│   ├── Grade MCQ and T&F automatically with Python logic
│   ├── Grade short answers using OpenAI API
│   │   ├── Call OpenAI API for each short answer
│   │   ├── Compare with expected answer using AI
│   │   └── Assign partial credit based on AI evaluation
│   ├── Calculate total score using Python calculations
│   ├── Generate feedback for each question
│   └── Return grading results
├── Save submission using SQLAlchemy
│   ├── id: UUID (generated)
│   ├── study_guide_id: study_guide_id
│   ├── user_id: user.id
│   ├── answers: List[Dict] (JSON field)
│   ├── score: float
│   ├── feedback: List[Dict] (JSON field)
│   └── submitted_at: datetime.utcnow()
└── Return: { data: { score, feedback, answers } }
```

**Request Body (Pydantic Schema):**
```python
class StudyGuideAnswer(BaseModel):
    question_id: str
    answer: str  # String for short answer

class StudyGuideSubmission(BaseModel):
    answers: List[StudyGuideAnswer] = Field(..., min_items=1)
```

**Success Response (Pydantic Schema):**
```python
class StudyGuideSubmissionResponse(BaseModel):
    score: float
    total_questions: int
    feedback: List[Dict[str, Any]]
    submitted_at: datetime
```

### 11.5 Delete Study Guide Flow

```
Frontend → DELETE /api/v1/study-guides/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch study guide from database using SQLAlchemy
├── Verify user owns the parent folder
├── Delete study guide and all submissions using SQLAlchemy CASCADE
└── Return: 204 No Content
```

---

## 12. Vector Database Operations

**🤖 Primary AI/ML Engineer Responsibility (with Backend Developer coordination on schema)**

### 12.1 Database Architecture

**PostgreSQL + pgvector Setup:**
- **PostgreSQL**: Stores structured data (users, folders, files, chat sessions, metadata)
- **pgvector**: Extension for storing and querying vector embeddings
- **Linkage**: Chat session IDs and file IDs link PostgreSQL records to vector embeddings

### 12.2 Text Chunking and Embedding Flow

```
Internal Process → Text Processing Pipeline
├── Receive text content from file upload or message
├── Text chunking with overlap strategy
│   ├── Split text into chunks (1000 characters each)
│   ├── Add overlap between chunks (200 characters)
│   ├── Preserve sentence boundaries where possible
│   └── Handle edge cases (tables, code blocks, etc.)
├── Generate embeddings using OpenAI API
│   ├── Call OpenAI Embeddings API (text-embedding-ada-002)
│   ├── Model: "text-embedding-ada-002" (1536 dimensions)
│   ├── Batch process multiple chunks for efficiency
│   └── Handle rate limiting and retry logic
├── Store embeddings in pgvector
│   ├── Create vector record with pgvector
│   │   ├── id: UUID (generated)
│   │   ├── embedding: vector(1536) (pgvector type)
│   │   ├── content: text (original chunk text)
│   │   ├── file_id: UUID (links to PostgreSQL file record)
│   │   ├── chunk_index: int (position in original text)
│   │   ├── metadata: JSONB (additional context)
│   │   └── created_at: timestamp
│   └── Create index for fast similarity search
├── Update PostgreSQL file record
│   ├── Add vector_ids: List[UUID] (references to pgvector)
│   ├── Update processing_status: 'completed'
│   └── Update vectors_count: int (number of chunks)
└── Return processing results for logging
```

### 12.3 Vector Similarity Search Flow

```
Internal Process → Semantic Search
├── Receive search query from chat/quiz/notes generation
├── Generate query embedding
│   ├── Call OpenAI Embeddings API with user query
│   ├── Use same model as document embeddings
│   └── Return query vector (1536 dimensions)
├── Perform pgvector similarity search
│   ├── Use cosine similarity operator (<=>)
│   ├── Filter by user access permissions
│   │   ├── JOIN with PostgreSQL file ownership
│   │   ├── WHERE files.folder.owner_id = user.id
│   │   └── AND files.deleted_at IS NULL
│   ├── Apply similarity threshold (e.g., > 0.7)
│   ├── Limit results (top-k, typically k=5-10)
│   └── ORDER BY similarity score DESC
├── Fetch related metadata from PostgreSQL
│   ├── JOIN vector results with file metadata
│   ├── Include file names, types, creation dates
│   └── Aggregate chunks by file for context
├── Prepare context for AI service
│   ├── Combine relevant chunks with metadata
│   ├── Include similarity scores for citation weighting
│   └── Format for LLM consumption
└── Return search results with metadata
```

**pgvector SQL Example:**
```sql
SELECT 
    v.id,
    v.content,
    v.file_id,
    v.chunk_index,
    f.name as file_name,
    1 - (v.embedding <=> $1) as similarity_score
FROM vector_embeddings v
JOIN files f ON v.file_id = f.id
JOIN folders fo ON f.folder_id = fo.id
WHERE fo.owner_id = $2
AND (1 - (v.embedding <=> $1)) > 0.7
ORDER BY v.embedding <=> $1
LIMIT 10;
```

### 12.4 Chat Session Vector Linking Flow

```
Chat Message Flow → Vector Context Integration
├── User sends message to chat space
├── Generate message embedding for similarity search
├── Perform vector similarity search (as above)
├── Store chat session data in PostgreSQL
│   ├── message_id: UUID (primary key)
│   ├── space_id: UUID (links to space)
│   ├── user_id: UUID (message sender)
│   ├── content: text (message content)
│   ├── role: enum ('user' | 'assistant')
│   ├── vector_chunks_used: UUID[] (pgvector references)
│   ├── similarity_scores: float[] (matching scores)
│   └── created_at: timestamp
├── Link vector chunks to chat session
│   ├── Create chat_vector_usage table entries
│   │   ├── chat_message_id: UUID (FK to messages)
│   │   ├── vector_embedding_id: UUID (FK to pgvector)
│   │   ├── similarity_score: float
│   │   ├── usage_type: enum ('context' | 'citation')
│   │   └── created_at: timestamp
│   └── Enable traceability from chat to source content
└── Return AI response with linked vector references
```

### 12.5 Vector Database Maintenance Flow

```
Background Tasks → Vector Database Optimization
├── Periodic vector cleanup
│   ├── Remove orphaned vectors (files deleted)
│   ├── Cleanup old chat session vector links
│   └── Vacuum and analyze pgvector indexes
├── Index optimization
│   ├── Rebuild pgvector indexes periodically
│   ├── Update index parameters for performance
│   └── Monitor index usage statistics
├── Embedding model updates
│   ├── Handle embedding model changes
│   ├── Migrate to new embedding dimensions
│   └── Batch re-embed existing content
└── Performance monitoring
    ├── Track vector search latency
    ├── Monitor embedding generation costs
    └── Optimize chunk size and overlap parameters
```

**Database Schema (SQLAlchemy Models):**
```python
# PostgreSQL Models
class VectorEmbedding(Base):
    __tablename__ = "vector_embeddings"
    
    id: UUID = Column(UUID, primary_key=True)
    embedding = Column(Vector(1536))  # pgvector type
    content: str = Column(Text)
    file_id: UUID = Column(UUID, ForeignKey("files.id"))
    chunk_index: int = Column(Integer)
    metadata: dict = Column(JSONB)
    created_at: datetime = Column(DateTime)

class ChatVectorUsage(Base):
    __tablename__ = "chat_vector_usage"
    
    id: UUID = Column(UUID, primary_key=True)
    chat_message_id: UUID = Column(UUID, ForeignKey("chat_messages.id"))
    vector_embedding_id: UUID = Column(UUID, ForeignKey("vector_embeddings.id"))
    similarity_score: float = Column(Float)
    usage_type: str = Column(Enum(UsageType))
    created_at: datetime = Column(DateTime)
```

**Request Body (Pydantic Schema):**
```python
class SearchEmbeddings(BaseModel):
    query_text: str = Field(..., min_length=1)
    model_name: Optional[str] = Field(None, max_length=255)
```

**Success Response (Pydantic Schema):**
```python
class SearchEmbeddingsResponse(BaseModel):
    search_results: List[Dict[str, Any]]
    user_id: str
    query_text: str
    model_name: str
    created_at: datetime
    
    class Config:
        from_attributes = True
```

### 12.6 Delete Embedding Flow

```
Frontend → DELETE /api/v1/vector-db/embeddings/{id}
├── Extract user from JWT token (Depends(get_current_user))
├── Fetch embedding from database using SQLAlchemy
├── Verify user owns the embedding
├── Delete embedding from vector database service
└── Return: 204 No Content
```

---

## 13. Error Handling Data Flows

**👤 Primary Backend Developer Responsibility (with AI/ML Engineer input on AI-specific errors)**

### 13.1 Validation Error Flow

```
Request → Pydantic Validation (Automatic)
├── Automatic field validation with Pydantic models
│   ├── Validate field presence and types
│   ├── Check field constraints (min/max length, etc.)
│   ├── Validate email format with EmailStr
│   └── Validate UUID format automatically
├── Custom validators with Pydantic @validator decorators
│   ├── Business logic validation
│   ├── Cross-field validation
│   └── Format-specific validation
├── If validation fails
│   ├── FastAPI automatically formats validation errors
│   ├── Return 422 Unprocessable Entity
│   └── Include detailed field-level errors
└── If validation passes: Continue to endpoint handler
```

**Error Response (FastAPI Standard):**
```python
# FastAPI automatically generates this format
{
  "detail": [
    {
      "loc": ["body", "email"],
      "msg": "value is not a valid email address",
      "type": "value_error.email"
    },
    {
      "loc": ["body", "password"],
      "msg": "ensure this value has at least 8 characters",
      "type": "value_error.any_str.min_length"
    }
  ]
}
```

### 13.2 Authentication Error Flow

```
Request → JWT Authentication (FastAPI Dependency)
├── Extract JWT token from Authorization header
│   ├── Check header format: "Bearer <token>"
│   └── Extract token value
├── Verify token using python-jose
│   ├── Decode JWT token with secret key
│   ├── Verify signature and algorithm
│   ├── Check expiration time
│   └── Validate token structure
├── Check token blacklist (if implemented)
├── If invalid token
│   ├── Raise HTTPException(status_code=401)
│   └── FastAPI returns standardized error
└── If valid token: Inject user into endpoint function
```

**Error Response (Custom HTTPException):**
```python
raise HTTPException(
    status_code=401,
    detail="Could not validate credentials",
    headers={"WWW-Authenticate": "Bearer"},
)
```

### 13.3 Authorization Error Flow

```
Request → Authorization Check (Custom Dependency)
├── Get current user from JWT dependency
├── Fetch resource from database using SQLAlchemy
├── Verify user ownership using Python logic
│   ├── Check owner_id field matches user.id
│   ├── Verify parent folder ownership through relationships
│   └── Validate access permissions
├── If not authorized
│   ├── Raise HTTPException(status_code=403)
│   └── Return standardized error response
└── If authorized: Continue to endpoint handler
```

**Error Response (Custom HTTPException):**
```python
raise HTTPException(
    status_code=403,
    detail="Not enough permissions"
)
```

### 13.4 Rate Limiting Flow

```
Request → Rate Limit Middleware (slowapi)
├── Extract user identifier (user ID or IP address)
├── Check user's request count using Redis or memory store
│   ├── Query rate limit store for current time window
│   ├── Count requests in last minute/hour
│   └── Check against configured limit
├── If limit exceeded
│   ├── Raise HTTPException(status_code=429)
│   ├── Include Retry-After header
│   └── Return error with retry information
└── If within limit: Continue to endpoint handler
```

**Error Response (slowapi):**
```python
{
  "detail": "Rate limit exceeded: 100 per 1 minute"
}
```

### 13.5 File Upload Error Flow

```
Request → File Upload Validation (FastAPI UploadFile)
├── Validate file size using custom dependency
│   ├── Check file.size against maximum (25MB)
│   └── If too large: Raise HTTPException(413)
├── Validate file type using custom validation
│   ├── Check MIME type from file.content_type
│   ├── Verify file extension
│   └── If unsupported: Raise HTTPException(415)
├── Check storage quota (if implemented)
│   ├── Calculate user's total storage using SQLAlchemy
│   ├── Compare with limit
│   └── If exceeded: Raise HTTPException(413)
└── If all validations pass: Process file upload
```

**Error Response (Custom HTTPException):**
```python
raise HTTPException(
    status_code=413,
    detail={
        "message": "File too large",
        "max_size": 26214400,  # 25MB in bytes
        "actual_size": file.size
    }
)
```

---

**Technology Stack Summary:**
- **Framework**: FastAPI with Python 3.9+
- **Database**: 
  - **PostgreSQL**: Primary database for structured data (SQLAlchemy ORM)
  - **pgvector**: PostgreSQL extension for vector embeddings storage and similarity search
- **Authentication**: python-jose for JWT, passlib for password hashing
- **Validation**: Pydantic models with automatic validation
- **AI Integration**: 
  - **OpenAI API**: GPT models for chat, quiz generation, notes, open-ended questions
  - **OpenAI Embeddings**: text-embedding-ada-002 for vector embeddings
  - **httpx**: Async HTTP client for AI service calls
- **Vector Operations**:
  - **pgvector**: Cosine similarity search with vector indexes
  - **Text Chunking**: Overlap-based content segmentation
  - **Semantic Search**: Vector similarity for content retrieval
- **File Processing**: 
  - **PyPDF2/pdfplumber**: PDF text extraction
  - **python-docx**: DOCX text extraction
  - **aiofiles**: Async file operations
- **New Features**:
  - **Open-ended Questions**: AI-powered long-form question generation and grading
  - **Flashcards**: Intelligent flashcard generation from content
  - **Study Guides**: Personalized study schedule creation and tracking
- **Testing**: pytest with httpx TestClient
- **Async Support**: Full async/await throughout the application

---

## 14. Integration & Deployment Coordination

**🤝 Joint Responsibility: Both developers must coordinate for successful integration**

### 14.1 Mock Development Strategy

**Backend Developer Mock Implementations:**
```python
# Mock AI services for independent development
class MockAIService:
    async def generate_quiz(self, file_ids: List[UUID], params: QuizParams) -> QuizData:
        return QuizData(questions=[{"id": "mock", "prompt": "Mock question"}])
    
    async def search_similar_content(self, query: str, user_id: UUID) -> List[SearchResult]:
        return [SearchResult(content="Mock content", similarity=0.9)]
```

**AI/ML Engineer Test Endpoints:**
```python
# Test endpoints for AI functionality
@router.post("/test/embeddings")
async def test_embedding_pipeline(text: str):
    # Test embedding generation and storage
    
@router.post("/test/similarity") 
async def test_similarity_search(query: str):
    # Test vector search functionality
```

### 14.2 Integration Testing Checkpoints

**Phase 1: Schema Validation**
- ✅ Database schema matches both teams' requirements
- ✅ API contracts agreed upon and documented
- ✅ Mock services return expected data formats

**Phase 2: Basic Integration**
- ✅ File upload triggers embedding generation
- ✅ Chat messages call AI service correctly
- ✅ Vector search returns formatted results

**Phase 3: End-to-End Testing**
- ✅ Complete user workflows function
- ✅ Error handling works across service boundaries
- ✅ Performance meets requirements

### 14.3 Production Deployment Checklist

**Backend Developer:**
- [ ] API endpoints deployed and health-checked
- [ ] Database migrations applied
- [ ] Authentication/authorization working
- [ ] File upload/storage configured

**AI/ML Engineer:**
- [ ] OpenAI API keys configured
- [ ] Vector embeddings pipeline operational
- [ ] pgvector indexes optimized
- [ ] AI service error handling tested

**Joint:**
- [ ] Integration tests passing
- [ ] Performance benchmarks met
- [ ] Monitoring and logging configured
- [ ] Documentation updated

---

This comprehensive data flow documentation provides a complete guide for implementing the EdutechHackathon backend API using Python and FastAPI with a split development team. It covers all endpoints, vector database operations, error handling, AI integration patterns, and coordination strategies for successful parallel development of enhanced educational features. 